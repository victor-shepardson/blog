<!DOCTYPE html>
<html prefix="
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="&gt; infernal knotted dark humming sunlight">
<meta name="viewport" content="width=device-width">
<title>~/blog</title>
<link href="assets/css/all.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Lato:900" rel="stylesheet">
<meta content="#323" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://victor-shepardson.github.io/blog/">
<link rel="next" href="index-1.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><style>
body {
  color: #ddd;
  background-color: #323;
}

a {
  color: #9ad;
}

h1, h2, h3, h4, h5, h6 {
  color: #eee;
}

strong {
  color: #dfdfdf;
}

code {
  padding: .25em .33em .1em .33em;
  font-size: 80%;
  color: #000;
  background-color: #b0a3b0;
  border-radius: 2px;
}

pre {
  background-color: #b0a3b0;
}

::-moz-selection { color: #000; background: #fff}
::selection { color: #000; }

blockquote {
  color: #bbb;
}

.post-title a {
  color: #9ad;
  font-family: "Lato"
}

.post-date {
  display: block;
  margin-top: -0.75rem;
  margin-bottom: 0.75rem;
  color: #767;
  font-size: 90%;
}

.pagination {
  color: #767;
}

.pagination-item {
  padding: .5em;
  border: 1px solid #767;
}

.tags {
  text-align: left;
  border: 0px dotted #eee;
}

.tag {
  display: inline-block;
  padding-top: .05em;
  padding-bottom: 0em;
  padding-left: .5em;
  padding-right: .5em;
  margin: 0px;
  border: 0px solid #ddd;
  background: #545;
  border-radius: 2px;
  font-size: smaller;
}

.tag:hover {
  background-color: #323;
}

div.hsidebar, .hsidebar {
  background-color: #212;
}

.sidebar-about h1 {
  font-family: "Lato", serif;
  font-size: 2.75rem;
  color: #eee;
}

.sidebar-about p {
  color: #878;
  line-height: 1.33;
  margin-bottom: 1em;
  margin-top: 1em
}

.copyright{
  color: #878
}
</style>
<link rel="prefetch" href="posts/pond-brain/" type="text/html">
</head>
<body class="">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://victor-shepardson.github.io/blog/">
                      <h1 id="brand"><a href="https://victor-shepardson.github.io/blog/" title="~/blog" rel="home">

        <span id="blog-title">~/blog</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">&gt; infernal knotted dark humming sunlight</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="https://victor-shepardson.github.io">~</a>
        <a class="sidebar-nav-item" href="archive.html">All Posts</a>
        <a class="sidebar-nav-item" href="categories/">Tags</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2024         <a href="mailto:victor.shepardson@gmail.com">Victor Shepardson</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/pond-brain/" class="u-url">Pond Brain (2023)</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2024-03-06T00:00:00-05:00" title="2024-03-06 00:00">2024-03-06 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>In 2023 I worked on <em>Pond Brain</em>, a pair of installations by <a href="https://www.instagram.com/jennasutela/">Jenna Sutela</a> for Copenhagen Contemporary and the <a href="https://helsinkibiennaali.fi/en/artist/jenna-sutela/">Helsinki Biennial</a>.</p>
<p>I trained real-time neural synthesis models with recordings of marine mammals and water sounds and built them into a feedback ecology with the sounds of musical instruments and stars in the Kepler exoplanet survey. Microphones in the room and attached to bronze water-singing bowls spun the sounds of visitors into the web of human, whale and celestial body.</p>
<p><img href="https://evermade-helsinkibiennaali-phase2-website.s3.eu-north-1.amazonaws.com/wp-content/uploads/2023/06/07143954/Jenna-Sutela_Pond-Brain-2.jpg"></p>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/intelligent-instruments/" class="u-url">Intelligent Instruments Lab</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2024-03-04T00:00:00-05:00" title="2024-03-04 00:00">2024-03-04 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>Since 2021 I'm a PhD researcher in the <a href="https://iil.is/">Intelligent Instruments Lab</a>.</p>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/synthesthesia/" class="u-url">Synesthesia Music Visualizer Scenes</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2024-03-03T00:00:00-05:00" title="2024-03-03 00:00">2024-03-03 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p><a href="http://synesthesia.live/">Synesthesia</a> is VJ software for making audio-reactive visuals. It draws from the <a href="https://www.shadertoy.com/">Shadertoy</a> community, where everything is made with GLSL fragment shaders, but includes an easy way to add audio and video inputs and build simple MIDI-mappable control interfaces. Basically, everything a VJ needs to actually perform live with some shaders. The folks building Synesthesia seem super respectful of free-sharing ethos of Shadertoy -- they reached out to shader programmers like me and comissioned new works for Synesthesia, while keeping the actual shaders always hackable by users within the application. I wound up making a few fun video feedback and cellular-automata based scenes, and recently was goaded into using some of them to mess up the live stream from an algorave concert at the AIMC conference: </p>
<!-- https://youtu.be/d0RMUqcbhmQ?t=8639 -->

<iframe width="921" height="726" src="https://www.youtube.com/embed/d0RMUqcbhmQ?start=8639" title="AIMC 2023 Concert 3 – Algorave" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/deviant-chain/" class="u-url">Deviant Chain (2019)</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2024-03-02T00:00:00-05:00" title="2024-03-02 00:00">2024-03-02 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>In 2019 I worked on a concert installation called <a href="https://stefanmaier.studio/deviant-chain-2019/"><em>Deviant Chain</em></a> with Stefan Maier and Alan Segal. Stefan, a composer, and Alan, a filmmaker, created a sort of art haunted house in which the audience moved from room to room in a former bank vault, encountering a series of short videos and sound works with themes of transhumanism and machine alterity. One of these dealt with the development of language and the human voice, and for this I created neural text-to-speech software for Stefan to gather sound material from. </p>
<p>At this time, the <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/">WaveNet demo</a> was in the air. This was one of the first neural generative models to work well with raw audio, and the uncanny babbling of the unconditional speech model was an arresting artifact of the ever-weirder capabilities of statistical autoregressive models. It had a few problems though: no open source implementation, very expensive training process, and extremely slow inference process. After some research, I found an open implementation of Tacotron2 from NVIDIA, which was state of the art for text-to-speech at the time, and substituted a WaveGlow vocoder for the original WaveNet. It turned out to still be quite slow and expensive to train WaveGlow, but it could actually generate audio with close to real-time throughput, and including a full text-to-speech model added many dimensions of control. This was a tool you could iterate with. I found the TTS could be induced to babble by letting it run past the end of its predicted alignment the the text, at which point it would get confused and start jumping around. Finally, I trained a few models on voices from Mozilla's <a href="https://commonvoice.mozilla.org/en">CommonVoice</a> project to get a much wider range of voice sounds. I built in some extra features to the CLI and got the whole thing running on Stefan's computer using PyInstaller. </p>
<p>I also developed a small glyph generator which was used in some of the videos -- this took in sounds, extracted features with some machine listening algorithms, and then mapped those features to a set of curves and strokes and stored them as 3D mesh files. 
I traveled to Oslo to install Deviant Chain, setting up the video players and cueing system. I was pretty happy with the results -- it was genuinely spooky working on it with just a few of us in the empty bank basement at night.</p>
<div class="video-wrapper">
  <div class="video">
    <iframe src="https://player.vimeo.com/video/431684120" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </div>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/reproducing-alpha-gan/" class="u-url">Reproducing α-GAN</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-05-06T00:00:00-04:00" title="2018-05-06 00:00">2018-05-06 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <!-- Generative Adversarial Networks or *GANs* are one the most exciting ideas to emerge from the deep learning boom.

Generally speaking, generation seems hard compared to classification. Think of images -- you can tell if an image has a bear in it. But drawing a convincing bear of your own is more difficult. To *classify* images as bear/non-bear is easy, to *generate* bears is hard. And indeed this is true for computers; feed a large neural network enough labeled images and it will be able to label new ones for you fairly well. But the best methods for generating original images will give you bizarre mutants or vague blurs.

GANs use two networks to transmute generation into classification. A *generator* (G) network turns random codes into images, and a *discriminator* (D) network classifies images as either real or generated. Each network provides an objective for the other -- G tries to fool D while the D learns from data to stay one step ahead. G and D can be any differentiable neural network, so G can be trained by backpropagation through D. In limited domains, this works [spooky good](https://www.youtube.com/watch?v=G06dEcZ-QTg&t=1m25s).

Ideally, a GAN gives you a dense code space from which you can sample random points on the data manifold. You can even take any two points in the code space and interpolate them to morph between images. Theoretically, any real (non-generated) image should have a corresponding point in the code space, but how do we find that point?

This is what Rosca et al. tackle in [Variational Approaches for Auto-Encoding
Generative Adversarial Networks](https://arxiv.org/abs/1706.04987) -->

<p>I implemented Rosca, Mihaela, et al. <a href="https://arxiv.org/abs/1706.04987">"Variational Approaches for Auto-Encoding
Generative Adversarial Networks"</a> using Pytorch. It's a modular implementation -- plug in any torch modules as encoder, generator, discriminator and code discriminator.</p>
<p>On <a href="https://github.com/victor-shepardson/alpha-GAN">GitHub</a></p>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/bear-cult-and-hofstadterpillar/" class="u-url">Bear Cult and HOFSTADTERPILLAR</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-04-19T00:00:00-04:00" title="2018-04-19 00:00">2018-04-19 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>These are two animations I made for FS35 with Jodie Mack in winter 2015.</p>
<h3>Bear Cult</h3>
<p><em>Bear Cult</em> was inspired by a collection of Joseph Campbell's essays I found in a used book store, specifically one entitled "Renewal Myths and Rites of the Primitive Hunters and Planters". It's about the prehistoric origins of myth. In Campbell's telling, the purpose of myth is to conquer death. Campbell cites preserved arrangements of cave bear bones as evidence for the ancient roots of a bear-baiting ritual still practiced by indigenous peoples in northern Japan. In the bear-baiting ritual, a young bear is captured, raised in captivity, and then ritually tormented, killed and eaten. The bear is believed to contain the soul of a demigod which yearns to be released from its fleshy bear-prison. For Campbell, this is the coping mechanism of an animal which kills to survive but understands death. It's the hunter's moral justification for killing: death isn't terminal, killing is a kindness. Ritualized death is a point of contact with the numinous; the fear of death is transmuted to awe. I found the deep connection Campbell makes between this sacred feeling and human capacity for cruelty heartbreaking.</p>
<p>I decided to depict a bear-baiting ritual. Not the specific cultural practice Campbell recounts, but the abstracted elements of a hypothetical prehistoric bear cult. The body of the bear would be made by hand with paper drawings and cutouts, while the soul would be made in software with <a href="posts/video-feedback">virtual video feedback</a>. These material and digital components would interact via fluorescent orange paper which could be keyed out in video editing software. A possible alternate title: "Mind-Body Chromakey". Originally I had storyboarded a longer narrative including human hands and spears piercing the bear and the escape of its soul, but only part proved feasible in the time I had.</p>
<p>I'm happy with how the bear soul material turned out. After shooting the paper animation, I exported just the key as a binary mask. As an input to the feedback process, the mask lets the roiling cloud-forms conform to the geometry of the bear eyes and nose-hole. Unfortunately the black-on-black paper was unwise; there's a lot of camera noise after adjusting the levels to make everything visible. A darker background of black felt or something might have worked better.</p>
<p>This project taught a lesson about how sound influences the perception of time in film. What felt like a good visual rhythm when I was silently animating seems weirdly abbreviated with the soundtrack added. The sound was a bit rushed, but I like the general effect. It's some recordings of bear vocalizations and carcass-mangling sounds chopped up into phase-rhythms with the same methods I used for <a href="posts/siba">SIBA</a>. I'd like to revisit this project someday.</p>
<div class="video-wrapper">
  <div class="video">
    <iframe src="https://player.vimeo.com/video/199104868" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </div>
</div>

<h3>HOFSTADTERPILLAR</h3>
<p><em>HOFSTADTERPILLAR</em> is a many-looped negotiation between rules, materials and intuition, an effort to explore Hofstadter's idea of <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> by drawing.</p>
<p>Drawing is an act of alternating abstraction and reification, recognition and construction of forms among materials. I draw, I see what I have drawn, I draw more. Animating by hand can be seen as a sort of <a href="posts/video-feedback">image feedback loop</a>; each frame is drawn with reference to previous frames, which are visible beneath transparent cels. I developed loose sets of rules to draw by, of the type "a silver line extends and curves more each frame". Other rules emerge from the materials; paint markers pool and smear when the next cel is placed on top. I used the "maximum cycle" technique of repeatedly drawing new material into the same set of frames to construct rich and evolving loops. The sound is another set of feedback loops, a Max doodle with chaotic noise generators stimulating banks of tuned filters.</p>
<div class="video-wrapper">
  <div class="video">
    <iframe src="https://player.vimeo.com/video/121756116" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </div>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/SIBA/" class="u-url">SIBA I|II|III</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-04-15T00:00:00-04:00" title="2018-04-15 00:00">2018-04-15 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>In 2014 I spent a lot of time working through a Steve Reich fascination by making tape music. This piece was eventually presented at ICMC 2015. SIBA I|II|III stands for "Studies In Being Alive One Two and Three", which was a joke about the lack of purpose I felt at the time and also a genuine attempt to describe what I was groping toward, stylized to reflect the asemic repetitious sound-destruction therein. I don't know how to feel about it.</p>
<iframe width="500" height="600" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/97523789&amp;color=%2399aadd&amp;auto_play=false&amp;hide_related=true&amp;show_comments=false&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true"></iframe>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/video-feedback/" class="u-url">Video Feedback</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-23T00:00:00-04:00" title="2017-04-23 00:00">2017-04-23 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>Douglas Hofstadter's <a href="https://archive.org/details/GEBen_201404">Gödel, Escher, Bach</a> contains about a hundred captivating ideas, and just one of them is the concept of video feedback: connect a video camera to a screen, and then point it at the screen. Weird stuff happens on the screen as light is projected though the air, captured by the camera and circled back to the screen, mutating each time. Zoom out to get a infinite hall of mirrors effect, zoom in to get kaleidoscopic pulsations, loosen the focus to get bubbling slime.</p>
<p>During college I became interested in computer graphics. Particularly, the idea of <em>procedural graphics</em>: producing images with only code, as opposed to virtually drawing (as with photoshop) or digitizing reality (as with a digital camera). Procedural graphics can translate some of the tantalizing infinites of mathematics into rich sensory experiences! <a href="http://www.iquilezles.org/www/articles/warp/warp.htm">A brief article</a> by Íñigo Quílez introduced me to the idea of turning a random number generator into an endless alien terrain. One popular way to do this is with <em>fragment shaders</em>, specialized programs for doing pixel-by-pixel graphics with the GPUs found in most computers. For more than a few examples, see <a href="https://www.shadertoy.com/">Shadertoy</a>.  </p>
<p>You can put a fragment shader into feedback just like a video camera. Pixels go in, pixels come out, pixels go back in 60 times per second. The shader determines how colors mutate and interact with nearby colors each time. Here are a few of mine:</p>
<iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/4d2BRm?gui=true&amp;t=10&amp;paused=true&amp;muted=false" allowfullscreen></iframe>
<iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lt2yz3?gui=true&amp;t=10&amp;paused=true&amp;muted=false" allowfullscreen></iframe>
<iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lsBcRK?gui=true&amp;t=10&amp;paused=true&amp;muted=false" allowfullscreen></iframe>

<p>Other things which resemble digital video feedback:</p>
<ul>
<li>2D Cellular automata like <a href="http://golly.sourceforge.net/">Conway's game of life</a>
</li>
<li>Finite element physical simulations of fluids and <a href="https://pmneila.github.io/jsexp/grayscott/">reaction-diffusion systems</a>
</li>
<li>Convolutional neural network visualization techniques like <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">deep dream</a>
</li>
</ul>
<p>Other people working with this kind of stuff:</p>
<ul>
<li><a href="http://sabrinaratte.com/">Sabrina Ratté</a></li>
<li><a href="http://alexanderdupuis.com/">Alex Dupuis</a></li>
<li><a href="https://twitter.com/cornusammonis">Cornus Ammonis</a></li>
<li><a href="https://pixlpa.com/">Andrew Benson</a></li>
<li><a href="https://twitter.com/Flexi23">Felix Woitzel</a></li>
</ul>
<p>I just can't stay away. See projects such as <a href="posts/abstract-concrete/">ABSTRACT/CONCRETE</a>, <a href="posts/convnet-video-feedback/">Video Synthesis With Convolutional Autoencoders</a> and <a href="posts/cortical-sonification">Data Sonification Using a Cortical Representation of Sound</a>.</p>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/abstract-concrete/" class="u-url">ABSTRACT/CONCRETE</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-01-11T00:00:00-05:00" title="2017-01-11 00:00">2017-01-11 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <!--
.. title: ABSTRACT/CONCRETE
.. slug: abstract-concrete
.. date: 2017-01-11
.. tags: sonic, visual, audiovisual, feedback, generative, openFrameworks
.. category:
.. link:
.. description:
.. type: text
-->
<style>
  .video-wrapper {
   width: 100%;
   display: inline-block;
   position: relative;
  }
  .video-wrapper:after {
      padding-top: 56.25%; /*16:9 ratio*/
      display: block;
      content: '';
  }
  .video {
      position: absolute;
      top: 0; bottom: 0; right: 0; left: 0;
  }
</style>
<div class="video-wrapper">
  <div class="video">
    <iframe src="https://player.vimeo.com/video/164777442" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </div>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/bendy/" class="u-url">Bendy: Wavetable Automata in Max</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-01-09T00:00:00-05:00" title="2017-01-09 00:00">2017-01-09 00:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>In spring of 2015 I made a synthesizer called Bendy as a seminar project. A technical description and a Max/MSP implementation can be found on <a href="https://github.com/victor-shepardson/bendy">GitHub</a>. Here's what is sounds like:</p>
<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/209732439&amp;color=%2399aadd&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>
    </div>
    </article>
</div>
        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-1.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>

    
    
        

</body>
</html>
