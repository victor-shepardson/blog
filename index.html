<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="the personal blog of Victor Shepardson">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Victor's Blog</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#dd00aa" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://victor-shepardson.github.io/blog/">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/bear-cult-and-hofstadterpillar/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://victor-shepardson.github.io/blog/">

                <span id="blog-title">Victor's Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>
                </li>
<li>
<a href="rss.xml">RSS feed</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    
<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/bear-cult-and-hofstadterpillar/" class="u-url">Bear Cult and HOFSTADTERPILLAR</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/bear-cult-and-hofstadterpillar/" rel="bookmark"><time class="published dt-published" datetime="2017-01-11T22:46:32-05:00" title="2017-01-11 22:46">2017-01-11 22:46</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>These are two animations I made for FS35 with Jodie Mack in winter 2015.</p>
<h3>Bear Cult</h3>
<p>Bear Cult was inspired by one article in a collection of Joseph Campbell's essays I found in a used book store. It's an introduction to his ideas about the monomyth and the ancient roots of culture. It was written in 1953 (I think) so I don't know how current the ideas are. Anyway, he describes two main branches of prehistoric religion: the bear cult and the cult of the fertility goddess. The fertility cult shows up in agricultural societies, the bear cult among hunter-gatherers. Campbell claims that preserved arrangements of cave bear bones are evidence for the ancient roots of a bear-baiting ritual still practiced by indigenous peoples in Siberia and northern Japan. In the bear-baiting ritual, a young bear is captured, raised in captivity, and then ritually tortured, killed and eaten. The bear is believed to contain the soul of a demigod which yearns to be released from its fleshy bear-prison. For Campbell, the bear cult is the coping mechanism of an animal which kills to survive but suddenly developed empathy for its prey. It's the prehistoric hunter's moral justification for killing: as a kindness and a means of contact with the numinous. The numinous is the sense of awe at contemplation of the infinite. We've all felt it, whether in a church or under the night sky: it's at the root of the religious and scientific impulses. It smells to me an awful lot like the notion of <em>intrinsic motivation</em> in reinforcement learning: a positive feeling obtained by beholding and questioning the world. The deep connection Campbell makes between this most sacred feeling and man's capacity for cruelty is utterly heartbreaking.</p>
<p>I set out to depict the bear-baiting ritual. The body of the bear would be made with paper drawings and cutouts, while the soul would be made with <a href="posts/video-feedback">virtual video feedback</a>. These material and digital components would interact via flourescent orange paper to be keyed out in adobe aftereffects. An good alternate tile might be "Mind-Body Chromakey". Originally I storyboarded a longer story including human hands and spears piercing the bear and the escape of its soul. Only the first "enter the bear" bit proved feasible for a midterm project. I'm very happy with how the bear soul material turned out. I exported the key as a binary mask and incorporated it in the feedback process, so the roiling cloud-forms corform to the geometry of the bear eyes and nose-hole. On the other hand the black-on-black paper was a terrible idea. I really needed black felt or something for the background. This project also taught me how sound influences the perception of time in film: what felt like a good visual rhythm when I was silently animating seems weirdly abbreviated with the sountrack added. The sound was a bit rushed, but I like the general effect. It's some recordings of bear vocalizations and some carcass-mangling sounds chopped up into phase-rhythms using Max. I would love to revisit this project some day with lessons learned.</p>
<div class="video-wrapper">
  <div class="video">
    <iframe src="https://player.vimeo.com/video/199104868" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </div>
</div>

<h3>HOFSTADTERPILLAR</h3>
<p>This one was my final project for the course. To be continued...</p>
<div class="video-wrapper">
  <div class="video">
    <iframe src="https://player.vimeo.com/video/121756116" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/video-feedback/" class="u-url">Video Feedback</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/video-feedback/" rel="bookmark"><time class="published dt-published" datetime="2017-01-11T22:08:22-05:00" title="2017-01-11 22:08">2017-01-11 22:08</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Way back in high school I read Douglas Hofstadter's <a href="https://archive.org/details/GEBen_201404">Gödel, Escher, Bach</a>. If you aren't familiar, I have to advise you to go read it instead of looking at my dumb blog. There are about a hundred captivating ideas in there, and just one of them is the concept of video feedback: connect a video camera to a screen, and then point it at the screen. Weird stuff happens on the screen. (Meanwhile I was learning to play the guitar, and discovering that the electro-acoustic feedback of wiggling the thing around in front of an amp is much more fun than drilling scales.)</p>
<iframe src="https://archive.org/stream/GEBen_201404/GEBen?ui=embed#page/n496/mode/1up" width="480px" height="430px" frameborder="0"></iframe>

<p>As an undergrad, I became interested in computer graphics. (I know of two things that teach you to really see: studying the science of computer graphics, and learning to paint or draw from life.) I was especially taken with the idea of <em>procedural graphics</em>: generating images from nothing but math, as opposed to using assets made by virtually drawing (as in photoshop) or digitizing reality (as with a digital camera). Procedural graphics can translate some of the tantalizing infinites of mathematics into rich sensory experiences! In particular <a href="http://www.iquilezles.org/www/articles/warp/warp.htm">this brief article</a> by Íñigo Quílez had me enthralled by the idea of turning a random number generator into an endless alien terrain.</p>
<p>Eventually I put two and two together: you can dispense with the video camera and program a completely virtual video feedback process. And once you do, it can do much weirder things than a video camera. This was hardly unknown in computer graphics, yet was weirdly unpopular. There's was some ideological preference for statelessness in the scene Quílez is part of, while in academic CG there tends to be focus on predictability and utility for efficient, photorealistic rendering. Then there are fractals, which are iterated, but not open ended: you iterate to convergence, rather than letting the image evolve as in video feedback. However <a href="http://csc.ucdavis.edu/~chaos/">Jim Crutchfield</a> has written about the dynamics of digital video feedback from a math/physics perspective. 2-D cellular automata like Conway's game of life can be thought of as digital video feedback, and there's a whole hobbyist community there, plus the academic artifical life community and <a href="http://www.wolframscience.com/">Stephen Wolfram</a>. The whole field of computational fluid dynamics is doing a similar thing with a different set of priorities! And of course video feedback shows up in fine art beginning (?) with video artists like Nam June Paik.</p>
<p>Finally in the fall of 2014 after encountering such cool dudes as <a href="http://sabrinaratte.com/">Sabrina Ratté</a> and <a href="http://www.alexanderdupuis.com/">Alex Dupuis</a> I was galvanized to get some feedback of my own going. Still blissfully ignorant of much, I started making stuff like this (more recent) doodle:</p>
<iframe width="100%" height="332" frameborder="0" src="https://www.shadertoy.com/embed/MdcSW8?gui=true&amp;t=10&amp;paused=false&amp;muted=false" allowfullscreen></iframe>

<p>(Incidentally <a href="https://synesthesia.live/">these folks</a> licensed the above shader to use in their music-reactive visuals software Synesthesia. It looks pretty neat, though I haven't been able to mess with it myself since I don't have a Mac. Maybe take a look if you do.)</p>
<p>Eventually this would grow into such projects as <a href="posts/abstract-concrete/">ABSTRACT/CONCRETE</a> and <a href="posts/convnet-video-feedback/">Video Synthesis With Convolutional Autoencoders</a> and</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/abstract-concrete/" class="u-url">ABSTRACT/CONCRETE</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/abstract-concrete/" rel="bookmark"><time class="published dt-published" datetime="2017-01-11T22:05:57-05:00" title="2017-01-11 22:05">2017-01-11 22:05</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<!--
.. title: ABSTRACT/CONCRETE
.. slug: abstract-concrete
.. date: 2017-01-11 22:05:57 UTC-05:00
.. tags:
.. category:
.. link:
.. description:
.. type: text
-->

<style>
  .video-wrapper {
   width: 100%;
   display: inline-block;
   position: relative;
  }
  .video-wrapper:after {
      padding-top: 56.25%; /*16:9 ratio*/
      display: block;
      content: '';
  }
  .video {
      position: absolute;
      top: 0; bottom: 0; right: 0; left: 0;
  }
</style>
<div class="video-wrapper">
  <div class="video">
    <iframe src="https://player.vimeo.com/video/164777442" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </div>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/mixer-supercollider-tidal-music/" class="u-url">Making Music With No-input Mixer, SuperCollider, and Tidal</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/mixer-supercollider-tidal-music/" rel="bookmark"><time class="published dt-published" datetime="2017-01-09T18:26:25-05:00" title="2017-01-09 18:26">2017-01-09 18:26</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>No-input Mixer</h3>
<p>Years ago I got one of <a href="http://usa.yamaha.com/products/live_sound/mixers/analog-mixers/mg102c/">these</a> cheap mixers so I could record and amplify dorm room nonsense music sessions. But at Dartmouth I had access to better recording facilities and the mixer was gathering dust. Also at Dartmouth, cool dude <a href="https://charlossound.wordpress.com/">Carlos Dominguez</a> introduced me to the concept of "no-input" mixing. Normally a mixer is the middleman between an instrument or microphone and a loudspeaker or recording device. It alters and facilitates sounds, but doesn't make sound. No-input mixing is total misuse of the mixer: you plug the mixer back into itself, as its own sole input. The mixer self-oscillates and makes its own sounds. It's the same principle as an electric guitar feeding back, or plugging a bunch of effects pedals together in a loop, or how analog synthesizers work under the hood. What's funny and exciting about the no-input mixer is how rich and diverse it sounds given that it isn't supposed to sound at all. The whole point of a mixer is to have fine control over routing and EQ; even a smallish stereo mixer has endless possible configurations. And because the oscillations depend on a delicate balance of amplification, no-input mixer is an enormously sensitive instrument. Playing one means moving a single knob <em>so slowly</em> to find the precise edge of chaos between two sounds; sweeping it <em>so quickly</em> to carve a tiny blip out of a squall; listening <em>so closely</em> to know when it's about to blow up. You learn the feel of a particular mixer, but it's still new every time. For some masterful no-input mixing, check out <a href="https://en.wikipedia.org/wiki/Toshimaru_Nakamura">Toshimaru Nakamura</a>.</p>
<p>As rich and surprisingly large as the space of feedback mixer sounds is, it's pretty distinctly within noise-drone-ambient world. You can make a lovely drone, you can make a wall of noise, you can make little squawking sounds, you can float through space. But I found myself yearning to hold on to one sound while searching for its perfect complement, or to condense a minute's worth of exploration into a short pattern, or to build whole stacks of sounds and rapidly switch between them. One option would be to ravenously <em>seek more channels</em>, but lacking a bigger mixer to abuse I turned to my old friend, computers. At first I just ran the mixer into Ableton. Simple stuff like grabbing a loop while continuing to tweak the mixer, or stacking a weird noisy tone into a weird noisy chord. Once the computer's in the loop, you can also expand the mixer's vocabulary by feeding processed sound back into the mixer. Slight <a href="https://en.wikipedia.org/wiki/Single-sideband_modulation">frequency shifting</a> of 0.1-10 Hz and reverb are particularly fertile. "Senseless Noise feat. Jeff" is an extremely goofy example of this setup, with me twirling the knobs and cool dude Jeff Mentch playfully tickling the default Ableton percussion synth. Also there are some blippy sine waves.</p>
<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/247613308&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>

<h3>Live Coding</h3>
<p>Using a big nasty rigid-yet-complex GUI like Ableton with the no-input mixer felt all wrong; it's nice for recording and has a powerful sampler, but it really spoils the elegance of the no-input mixer. And it demands the extra moving part of a MIDI controller or at least a mouse. As an alternative I've finally been getting into live coding. The usual way of programming involves writing code in a text file, and then running it all at once. Maybe the program halts and you inspect the result, or maybe it runs in a loop doing something until you close it. Only then can you can alter the code and try again. This is not an ideal paradigm for making music with code. A better one would be to have the program running as you write and revise it bit by bit, and have each change reflected in what you hear. Then instead of trying to program music ahead of time, the programming <em>is</em> the music. That's the idea of live coding. Some <a href="https://toplap.org/about/">exponents of live coding</a> are really into the performance angle, putting the programmer on a stage and projecting code for an audience to see.</p>
<p>Lately I've been learning to use two live-coding music tools, <a href="http://supercollider.github.io/">SuperCollider</a> for audio and <a href="https://tidalcycles.org/">Tidal</a> for patterns.</p>
<p>SuperCollider is three things: a flexible software synthesizer <em>scsynth</em>, a scripting language <em>sclang</em> to control it, and a development environment <em>scide</em>. With SuperCollider, you build synthesizers as graphs of unit generators which do simple things like generate a sine wave, delay a signal, apply a filter. Sclang can compactly do things which are tedious in a program like Max or Ableton and impossible with hardware, like "make 100 versions of this sound and spread them across the stereo field" or "randomize all the connections between these synthesizers". It does come with a steep learning curve. There are always several programming paradigms and syntax options to choose from; the separation between scsynth and sclang takes a while to wrap your head around; scsynth and scide can be temperamental; errors can be uninformative. It's enough to drive you off when you're starting out and just want to make a weird chord out of 100 sine waves. Nevertheless, I've been getting the hang of it and having a blast dreaming up wacky delay effects to use with the mixer.</p>
<p>Tidal is complementary to SuperCollider. It doesn't deal directly with sound, but focuses on music as a sequence of discrete events. Like Ableton Live, it enshrines pulse: everything is a loop. Very unlike Ableton, it brings all the machinery of maximally-dorky pure functional programming to bear on the definition of musical patterns. Tidal is two things: a sublanguage of <a href="https://www.haskell.org/">Haskell</a>, and a runtime which maintains a clock and emits OSC messages. With Tidal, you write a simple pattern as something like <code>sound "bd sn bd sn"</code> meaning "kick, snare, kick, snare". The part in quotes isn't a string, but Tidal's special pattern literal. You construct different rhythms using nesting and rests, e.g. <code>"hh [hh [~ hh]]"</code>. Immediately, I liked this much better than clicking around a piano roll (though it's obviously very different from playing on a controller). You can then pattern other parameters by adding something like <code># pan "0 1" # gain "0.1 0.2 0.3"</code>. Each pattern can have its own meter. You can transform a pattern by e.g. changing the speed; you can superimpose or concatenate lists of patterns. And since Tidal is a <em>language</em>, you can do all that recursively, building complex patterns up of simple atoms, inventing and reusing processes. Tidal can send arbitrary MIDI and OSC, but it's really made to use with its own special sampler called Dirt (which has a SuperCollider implementation). I've been recording trajectories of a few minutes through computer-augmented no-input mixer space, then using Tidal+Dirt to chop them up into patterns. "Improv December 17" is made from one long mixer recording and a bass drum sample that comes with Dirt.</p>
<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/298537508&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>

<p>Using Tidal is a lot like programming in Haskell, because that's what it is. It feels natural to work with lists and higher order functions, but deeply weird when you want to do something in an imperative style. Haskell is sort of an odd match for live coding. In the middle of a groove, the last thing I want to worry about is whether to use <code>fromRational</code> or <code>floor</code> to make sure some number is the right type to play nice with a Tidal function. On the other hand, Haskell and the ingenious pattern literal make for an incredibly concise syntax. And though Haskell's strong type safety can add cognitive load I'd rather spend on musical matters, it also makes it harder to break: a mistake is more likely to be rejected by the interpreter (doing nothing) than to do something unexpected to the sound.</p>
<p>Tidal and SuperDirt are pretty experimental software, and there are some rough edges. Not all the documentation is there, and some things just aren't working for me. Sometimes samples don't fade in right and pop, the built in delay effect is acting bizarrely, and it can be awkward to deal with long samples. Right off the bat, I had to build the latest version to fix a fatal bug. There are some sampler features I miss like having expressive envelopes; I'd also like more flexible EQ. If I can get fluent at SuperCollider, I may try to implement some of these things myself.</p>
<p>So far all my SuperCollider and Tidal work is live-coded spaghetti, but eventually I hope to pack some of it into nice libraries. Stay tuned!</p>
<p>Side note: lately I've been using an Ubuntu desktop for music. To record, edit and master material coming out of SuperCollider, I tried out <a href="https://ardour.org/">Ardour</a> for the first time in years. I was impressed by everything until I discovered that automation curves wouldn't draw right. So close! I also tried out the free <a href="http://calf-studio-gear.org/">Calf plugins</a>, which are very flexible and sound great. Seems they've been around for a while, but I never knew about them before. The multiband compressor and gate effects worked well for massaging the sound of a dense stereo recording.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/cortical-sonification/" class="u-url">Data Sonification Using a Cortical Representation of Sound</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/cortical-sonification/" rel="bookmark"><time class="published dt-published" datetime="2017-01-09T18:26:25-05:00" title="2017-01-09 18:26">2017-01-09 18:26</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <p><a href="https://github.com/victor-shepardson/video-feedback-sonification">GitHub</a></p>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/convnet-video-feedback/" class="u-url">Video Synthesis With Convolutional Autoencoders</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/convnet-video-feedback/" rel="bookmark"><time class="published dt-published" datetime="2017-01-09T18:26:25-05:00" title="2017-01-09 18:26">2017-01-09 18:26</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Context: see <a href="posts/video-feedback">my post on video feedback</a></p>
<p>In spring of 2015 I took a <a href="http://www.cs.dartmouth.edu/~lorenzo/teaching/cs189/Archive/Spring2015/">seminar in deep learning</a> with Lorenzo Torresani. It was one of the most exciting classes I've ever taken. I had been introduced to the field of machine learning by Lorenzo's class the previous fall, and was already riding high on the concept of solving generic problems by optimization. The idea of deep learning for <em>representation learning</em>--extending ML to more generic problems by learning to interpret raw data--was exciting on its own. I'd also been talking a lot to cool dudes <a href="http://pkmital.com/home/">Parag Mital</a>, and <a href="http://www.cs.dartmouth.edu/~sarroff/">Andy Sarroff</a> about their work with machine learning, sound and video. And what really blew my mind about deep learning was the similarity between neural networks and the audio/video feedback I'd been using to make noise. This project was my attempt to incorporate a convolutional network trained to encode images as part of a video feedback process.</p>
<p>I really bit off more than I could chew for a seminar project: my approach demanded that I design and train my own neural network with caffe <em>and</em> re-implement the forward pass with OpenGL <em>and</em> spend time exploring the feedback behavior. I was able to train my autoencoders with some success, and I was able to make some singular boiling multicolored nonsense. I didn't get the spectacular emergence of natural image qualities I hoped for. A few months later deep dream came out, articulating a clearly superior approach with jaw-dropping results. I may still try to get the openFrameworks app running again and capture some video, for posterity.</p>
<p>Here's the <a href="https://github.com/victor-shepardson/feature-feedback">GitHub</a>, which includes a <a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/writeup.ipynb">writeup</a>, a <a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/presentation.ipynb">jupyter notebook</a> with the autoencoder experiments in it, and the source code for an openFrameworks app which runs the process interactively.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/bendy/" class="u-url">Bendy: Wavetable Automata in Max</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/bendy/" rel="bookmark"><time class="published dt-published" datetime="2017-01-09T18:26:25-05:00" title="2017-01-09 18:26">2017-01-09 18:26</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>In spring of 2015 I made a synthesizer called Bendy as a seminar project. A technical description and a Max/MSP implementation can be found on <a href="https://github.com/victor-shepardson/bendy">GitHub</a>. Here's what is sounds like:</p>
<iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/209732439&amp;color=ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false"></iframe>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/hello/" class="u-url">Hello!</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Victor Shepardson
            </span></p>
            <p class="dateline"><a href="posts/hello/" rel="bookmark"><time class="published dt-published" datetime="2017-01-06T17:40:25-05:00" title="2017-01-06 17:40">2017-01-06 17:40</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>For 2017 I resolve to <strong>document more</strong>. I'll be reflecting on old projects here and posting about current ones.</p>
<!-- TEASER_END -->

<p>Things to keep in mind: why am I writing these posts? Who is the audience? As a whole, this blog should give my future self and anyone else an idea of what I've accomplished. Individually, each post should connect some interesting dots for whomever reads it. A good rule of thumb is probably: machine learning people should be able to understand a post about music, and musicians should be able to understand a post about ML. My mom should be able to read any post and get the gist. Writing concise explanations is a real challenge, but a rewarding one! But it's definitely overkill to explain what deep learning is in every related post...I should link to an explainer post or include it as a sidebar, maybe?</p>
<p>I'd like to organize posts by project, and maybe tag them by topic. Right now the site generator is just putting them in most-recently-edited order.</p>
<p>I am dumb and think it's funny to introduce everyone I know personally as a "cool dude". If anyone objects to being referred to this way, please contact me and provide a preferred title!</p>
</div>
    </div>
    </article>
</div>







        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2017         <a href="mailto:victor.shepardson@gmail.com">Victor Shepardson</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
