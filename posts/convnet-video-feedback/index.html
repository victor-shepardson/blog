<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>~/blog · Video Synthesis With Convolutional Autoencoders </title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Lato:900" rel="stylesheet">
<meta content="#323" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><style>
body {
  color: #ddd;
  background-color: #323;
}

a {
  color: #9ad;
}

h1, h2, h3, h4, h5, h6 {
  color: #eee;
}

strong {
  color: #dfdfdf;
}

code {
  padding: .25em .33em .1em .33em;
  font-size: 80%;
  color: #000;
  background-color: #b0a3b0;
  border-radius: 2px;
}

pre {
  background-color: #b0a3b0;
}

::-moz-selection { color: #000; background: #fff}
::selection { color: #000; }

blockquote {
  color: #bbb;
}

.post-title a {
  color: #9ad;
  font-family: "Lato"
}

.post-date {
  display: block;
  margin-top: -0.75rem;
  margin-bottom: 0.75rem;
  color: #767;
  font-size: 90%;
}

.pagination {
  color: #767;
}

.pagination-item {
  padding: .5em;
  border: 1px solid #767;
}

.tags {
  text-align: left;
  border: 0px dotted #eee;
}

.tag {
  display: inline-block;
  padding-top: .05em;
  padding-bottom: 0em;
  padding-left: .5em;
  padding-right: .5em;
  margin: 0px;
  border: 0px solid #ddd;
  background: #545;
  border-radius: 2px;
  font-size: smaller;
}

.tag:hover {
  background-color: #323;
}

div.hsidebar, .hsidebar {
  background-color: #212;
}

.sidebar-about h1 {
  font-family: "Lato", serif;
  font-size: 2.75rem;
  color: #eee;
}

.sidebar-about p {
  color: #878;
  line-height: 1.33;
  margin-bottom: 1em;
  margin-top: 1em
}

.copyright{
  color: #878
}
</style>
<meta name="author" content="Victor Shepardson">
<link rel="prev" href="../cortical-sonification/" title="Data Sonification Using a Cortical Representation of Sound" type="text/html">
<link rel="next" href="../bendy/" title="Bendy: Wavetable Automata in Max" type="text/html">
<meta property="og:site_name" content="~/blog">
<meta property="og:title" content="Video Synthesis With Convolutional Autoencoders">
<meta property="og:url" content="https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/">
<meta property="og:description" content="This project was my attempt to incorporate a neural network trained to encode images into a video feedback process.
In spring of 2015 I took a seminar in deep learning which got me real excited. Machi">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-01-09T18:26:25-05:00">
<meta property="article:tag" content="autoencoder">
<meta property="article:tag" content="caffe">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="lorenzo torresani">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="openFrameworks">
<meta property="article:tag" content="recurrent">
<meta property="article:tag" content="video feedback">
<meta property="article:tag" content="visual">
</head>
<body class="">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://victor-shepardson.github.io/blog/">
                      <h1 id="brand"><a href="https://victor-shepardson.github.io/blog/" title="~/blog" rel="home">

        <span id="blog-title">~/blog</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">&gt; infernal knotted dark humming sunlight</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../">~</a>
        <a class="sidebar-nav-item" href="../../archive.html">All Posts</a>
        <a class="sidebar-nav-item" href="../../categories/">Tags</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2018         <a href="mailto:victor.shepardson@gmail.com">Victor Shepardson</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><h1 class="post-title p-name"><a href="." class="u-url">Video Synthesis With Convolutional Autoencoders</a></h1>

    <span class="post-date">
      <time class="published dt-published" datetime="2017-01-09T18:26:25-05:00" itemprop="datePublished" title="2017-01-09 18:26">2017-01-09 18:26</time></span>

    
    

    <div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This project was my attempt to incorporate a neural network trained to encode images into a <a href="../video-feedback">video feedback process</a>.</p>
<p>In spring of 2015 I took a <a href="http://www.cs.dartmouth.edu/~lorenzo/teaching/cs189/Archive/Spring2015/">seminar in deep learning</a> which got me real excited. Machine learning is the study of general methods for problem solving using optimization and data. Deep learning is a particular approach to ML using models with many differentiable layers of parameters. Especially interesting to me was <em>representation learning</em>: using ML methods to extract meaningful features from "raw" data like the pixels of an image. And I'd been talking a lot to <a href="http://pkmital.com/home/">Parag Mital</a> and <a href="http://www.cs.dartmouth.edu/~sarroff/">Andy Sarroff</a> about their respective work with deep learning, sound and video. But what freaked me out the most about deep learning was the similarity between neural networks and the audio/video feedback I'd been using to make noise.</p>
<p>The kind of digital video feedback I'd been playing with was superficially like a recurrent neural network. At each time step, the current frame of video would be computed from the last (and optionally, the current frame of an input video). There would first be some linear function from images to images, like translation or blurring; generally, each pixel would take on a linear combination of pixels in the last frame and input frame. Then, there would be some pixel-wise bounded nonlinearity to keep the process from blowing up, like wrapping around [0, 1] or sigmoid squashing. That's the architecture of an RNN. The only difference was that rather than represent the linear transformation as a big ol' parameter matrix, I would hand-craft it from a few sampling operations in a fragment shader. And instead of training by backpropagation to do some task, I would fiddle with it manually until it had visually interesting dynamics.</p>
<p>I might have stopped there and tried to make my video-RNN parameters trainable. But to do what? It was pretty clear I wouldn't make much headway on synthesis of natural video in two weeks, without experience in deep learning software frameworks, and without even a GPU to run on. I wanted a toy-sized problem which might still result in a cool interactive video process. So I came up with a different approach: rather than try to train a recurrent network I would train a feedforward convolutional network, then transplant its parameters into a still partially hand-constructed video process. I came up with a neat way to do that: my CNN would be arranged as an autoencoder. It would have an hourglass shape, moving information out of 2-D image space and into a dense vector representation (which I vaguely hoped would make the network implement a "hierarchy of abstraction"). This would mean that I could bolt an "abstraction dimension" onto the temporal and spatial dimensions of a video feedback process. The autoencoder would implement "texture sampling" from the "less abstract" layer below and "more abstract" layer above. Then I could fiddle with the dynamics by implementing something like "each layer approaches the previous time-step minus the layer above plus the layer below, squashed".</p>
<p>I almost bit off more than I could chew for a seminar project: my approach demanded that I design and train my own neural network with caffe <em>and</em> re-implement the forward pass with OpenGL <em>and</em> spend time exploring the resultant dynamics. I was able to train my autoencoders on <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR</a> with some success, and I was able to make some singular boiling multicolored nonsense. But I didn't get the spectacular emergence of natural image qualities I hoped for.</p>
<p>Here's the <a href="https://github.com/victor-shepardson/feature-feedback">GitHub</a>, which includes a <a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/writeup.ipynb">technical writeup</a>, a <a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/presentation.ipynb">jupyter notebook</a> with the autoencoder experiments in it, and the (probably very brittle) source code for an openFrameworks app which runs the process interactively, optionally with webcam input. It's based on early 2015 versions of caffe and openFrameworks. I may still try to get the openFrameworks app running again and capture some video, for posterity.</p>
<p>A few months later <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">deep dream</a> came out. Deep dream does a similar thing: it iteratively alters an image using a pre-trained CNN to manifest natural image qualities. The trick to deep dream is that the mechanism is the same as training the network, optimizing inputs instead of parameters. Vanilla deep dream converges, but it's simple to make a <a href="https://www.youtube.com/watch?v=IREsx-xWQ0g">dynamic version</a> by incorporating infinite zoom or similar. Too bad I didn't get into the filter visualization papers for this project -- I failed to realize that backpropagation could do exactly what I wanted!</p>
</div>
    </div>
    <aside class="postpromonav"><nav><p itemprop="keywords" class="tags">
            <span class="tag"><a class="p-category" href="../../categories/autoencoder/" rel="tag">autoencoder</a></span>
            <span class="tag"><a class="p-category" href="../../categories/caffe/" rel="tag">caffe</a></span>
            <span class="tag"><a class="p-category" href="../../categories/deep-learning/" rel="tag">deep learning</a></span>
            <span class="tag"><a class="p-category" href="../../categories/lorenzo-torresani/" rel="tag">lorenzo torresani</a></span>
            <span class="tag"><a class="p-category" href="../../categories/machine-learning/" rel="tag">machine learning</a></span>
            <span class="tag"><a class="p-category" href="../../categories/openframeworks/" rel="tag">openFrameworks</a></span>
            <span class="tag"><a class="p-category" href="../../categories/recurrent/" rel="tag">recurrent</a></span>
            <span class="tag"><a class="p-category" href="../../categories/video-feedback/" rel="tag">video feedback</a></span>
            <span class="tag"><a class="p-category" href="../../categories/visual/" rel="tag">visual</a></span>
      </p>

            <div class="pager hidden-print pagination">

            <span class="previous pagination-item older">
                <a href="../cortical-sonification/" rel="prev" title="Data Sonification Using a Cortical Representation of Sound">
                Previous post
                </a>
            </span>


            <span class="next pagination-item newer">
                <a href="../bendy/" rel="next" title="Bendy: Wavetable Automata in Max">
Next post
              </a>
            </span>

        </div>

    </nav></aside></article>
</div>

    
    
        

</body>
</html>
