<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>~/blog (Posts about visual)</title><link>https://victor-shepardson.github.io/blog/</link><description></description><atom:link href="https://victor-shepardson.github.io/blog/categories/visual.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 24 Apr 2018 03:27:30 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Bear Cult and HOFSTADTERPILLAR</title><link>https://victor-shepardson.github.io/blog/posts/bear-cult-and-hofstadterpillar/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;These are two animations I made for FS35 with Jodie Mack in winter 2015.&lt;/p&gt;
&lt;h3&gt;Bear Cult&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Bear Cult&lt;/em&gt; was inspired by a collection of Joseph Campbell's essays I found in a used book store, specifically one entitled "Renewal Myths and Rites of the Primitive Hunters and Planters". It's about the prehistoric origins of myth. In Campbell's telling, the purpose of myth is to conquer death. Campbell cites preserved arrangements of cave bear bones as evidence for the ancient roots of a bear-baiting ritual still practiced by indigenous peoples in northern Japan. In the bear-baiting ritual, a young bear is captured, raised in captivity, and then ritually tormented, killed and eaten. The bear is believed to contain the soul of a demigod which yearns to be released from its fleshy bear-prison. For Campbell, this is the coping mechanism of an animal which kills to survive but understands death. It's the hunter's moral justification for killing: death isn't terminal, killing is a kindness. Ritualized death is a point of contact with the numinous; the fear of death is transmuted to awe. I found the deep connection Campbell makes between this sacred feeling and human capacity for cruelty heartbreaking.&lt;/p&gt;
&lt;p&gt;I decided to depict a bear-baiting ritual. Not the specific cultural practice Campbell recounts, but the abstracted elements of a hypothetical prehistoric bear cult. The body of the bear would be made by hand with paper drawings and cutouts, while the soul would be made in software with &lt;a href="https://victor-shepardson.github.io/blog/posts/video-feedback"&gt;virtual video feedback&lt;/a&gt;. These material and digital components would interact via fluorescent orange paper which could be keyed out in video editing software. A possible alternate title: "Mind-Body Chromakey". Originally I had storyboarded a longer narrative including human hands and spears piercing the bear and the escape of its soul, but only part proved feasible in the time I had.&lt;/p&gt;
&lt;p&gt;I'm happy with how the bear soul material turned out. After shooting the paper animation, I exported just the key as a binary mask. As an input to the feedback process, the mask lets the roiling cloud-forms conform to the geometry of the bear eyes and nose-hole. Unfortunately the black-on-black paper was unwise; there's a lot of camera noise after adjusting the levels to make everything visible. A darker background of black felt or something might have worked better.&lt;/p&gt;
&lt;p&gt;This project taught a lesson about how sound influences the perception of time in film. What felt like a good visual rhythm when I was silently animating seems weirdly abbreviated with the soundtrack added. The sound was a bit rushed, but I like the general effect. It's some recordings of bear vocalizations and carcass-mangling sounds chopped up into phase-rhythms with the same methods I used for &lt;a href="https://victor-shepardson.github.io/blog/posts/siba"&gt;SIBA&lt;/a&gt;. I'd like to revisit this project someday.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/199104868" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3&gt;HOFSTADTERPILLAR&lt;/h3&gt;
&lt;p&gt;This one was my final project for the course. To be continued...&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/121756116" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>aftereffects</category><category>animation</category><category>bear cult</category><category>douglas hofstadter</category><category>feedback</category><category>hofstadterpillar</category><category>jodie mack</category><category>joseph campbell</category><category>max</category><category>sonic</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/bear-cult-and-hofstadterpillar/</guid><pubDate>Thu, 12 Jan 2017 03:46:32 GMT</pubDate></item><item><title>Video Feedback</title><link>https://victor-shepardson.github.io/blog/posts/video-feedback/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;Douglas Hofstadter's &lt;a href="https://archive.org/details/GEBen_201404"&gt;Gödel, Escher, Bach&lt;/a&gt; contains about a hundred captivating ideas, and just one of them is the concept of video feedback: connect a video camera to a screen, and then point it at the screen. Weird stuff happens on the screen as light is projected though the air, captured by the camera and circled back to the screen, mutating each time. Zoom out to get a infinite hall of mirrors effect, zoom in to get kaleidoscopic pulsations, loosen the focus to get bubbling slime.&lt;/p&gt;
&lt;p&gt;As an undergrad, I became interested in computer graphics. Particularly, the idea of &lt;em&gt;procedural graphics&lt;/em&gt;: generating images from nothing but math, as opposed to using assets made by virtually drawing (as with photoshop) or digitizing reality (as with a digital camera). Procedural graphics can translate some of the tantalizing infinites of mathematics into rich sensory experiences! &lt;a href="http://www.iquilezles.org/www/articles/warp/warp.htm"&gt;This brief article&lt;/a&gt; by Íñigo Quílez introduced me to the idea of turning a random number generator into an endless alien terrain. One popular way to do this is with &lt;em&gt;fragment shaders&lt;/em&gt;, specialized programs for doing pixel-by-pixel graphics with the GPUs found in most computers. For more than a few examples, see &lt;a href="https://www.shadertoy.com/"&gt;Shadertoy&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;You can put a fragment shader into feedback just like a video camera. Pixels go in, pixels come out, pixels go back in 60 times per second. The shader determines how colors mutate and interact with nearby colors each time. Here are a few of mine:&lt;/p&gt;
&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/4d2BRm?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;

&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lt2yz3?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;

&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lsBcRK?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;A few other things which resemble digital video feedback:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D Cellular automata like &lt;a href="http://golly.sourceforge.net/"&gt;Conway's game of life&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finite element physical simulations of fluids and &lt;a href="https://pmneila.github.io/jsexp/grayscott/"&gt;reaction-diffusion systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A few other people working with this kind of stuff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sabrinaratte.com/"&gt;Sabrina Ratté&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://alexanderdupuis.com/"&gt;Alex Dupuis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/cornusammonis"&gt;Cornus Ammonis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pixlpa.com/"&gt;Andrew Benson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/Flexi23"&gt;Felix Woitzel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I just can't stay away. See projects such as &lt;a href="https://victor-shepardson.github.io/blog/posts/abstract-concrete/"&gt;ABSTRACT/CONCRETE&lt;/a&gt;, &lt;a href="https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/"&gt;Video Synthesis With Convolutional Autoencoders&lt;/a&gt; and &lt;a href="https://victor-shepardson.github.io/blog/posts/cortical-sonification"&gt;Data Sonification Using a Cortical Representation of Sound&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>feedback</category><category>generative</category><category>shader</category><category>video</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/video-feedback/</guid><pubDate>Thu, 12 Jan 2017 03:08:22 GMT</pubDate></item><item><title>ABSTRACT/CONCRETE</title><link>https://victor-shepardson.github.io/blog/posts/abstract-concrete/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;!--
.. title: ABSTRACT/CONCRETE
.. slug: abstract-concrete
.. date: 2017-01-11 22:05:57 UTC-05:00
.. tags: sonic, visual, audiovisual, feedback, generative, openFrameworks
.. category:
.. link:
.. description:
.. type: text
--&gt;

&lt;style&gt;
  .video-wrapper {
   width: 100%;
   display: inline-block;
   position: relative;
  }
  .video-wrapper:after {
      padding-top: 56.25%; /*16:9 ratio*/
      display: block;
      content: '';
  }
  .video {
      position: absolute;
      top: 0; bottom: 0; right: 0; left: 0;
  }
&lt;/style&gt;

&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/164777442" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>audiovisual</category><category>feedback</category><category>generative</category><category>openFrameworks</category><category>sonic</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/abstract-concrete/</guid><pubDate>Thu, 12 Jan 2017 03:05:57 GMT</pubDate></item><item><title>Data Sonification Using a Cortical Representation of Sound</title><link>https://victor-shepardson.github.io/blog/posts/cortical-sonification/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;&lt;a href="https://github.com/victor-shepardson/video-feedback-sonification"&gt;GitHub&lt;/a&gt;&lt;/p&gt;</description><category>audiovisual</category><category>matlab</category><category>michael casey</category><category>neuroscience</category><category>sonic</category><category>sonification</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/cortical-sonification/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Video Synthesis With Convolutional Autoencoders</title><link>https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;Context: see my &lt;a href="https://victor-shepardson.github.io/blog/posts/video-feedback"&gt;post on video feedback&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In spring of 2015 I took a &lt;a href="http://www.cs.dartmouth.edu/~lorenzo/teaching/cs189/Archive/Spring2015/"&gt;seminar in deep learning&lt;/a&gt; with Lorenzo Torresani. It was one of the most exciting classes I've ever taken. I had been introduced to the field of machine learning by Lorenzo's class the previous fall, and was already riding high on the concept of solving generic problems by optimization. The idea of deep learning for &lt;em&gt;representation learning&lt;/em&gt;--extending ML to more generic problems by learning to interpret raw data--was exciting on its own. I'd also been talking a lot to cool dudes &lt;a href="http://pkmital.com/home/"&gt;Parag Mital&lt;/a&gt;, and &lt;a href="http://www.cs.dartmouth.edu/~sarroff/"&gt;Andy Sarroff&lt;/a&gt; about their work with machine learning, sound and video. And what really blew my mind about deep learning was the similarity between neural networks and the audio/video feedback I'd been using to make noise. This project was my attempt to incorporate a convolutional network trained to encode images as part of a video feedback process.&lt;/p&gt;
&lt;p&gt;The kind of digital video feedback I'd been playing with was superficially quite a bit like a recurrent neural network. At each time step, the current frame of video would be computed from the last (and optionally, the current frame of an input video). There would first be some linear function from images to images (call it D), like translation or blurring; generally, each pixel would take on a linear combination of pixels in the last frame and input frame. Then, there would be some pixel-wise bounded nonlinearity (call it σ) to keep the process from blowing up, like wrapping around [0, 1] or sigmoid squashing. That's the architecture of an RNN. The only difference was that rather than represent the linear transformation D as a big ol' parameter matrix, I would hand-craft it from a few sampling operations in a fragment shader. And instead of training by backpropagation to do some task, I would fiddle with it manually until it had visually interesting dynamics.&lt;/p&gt;
&lt;p&gt;I might have stopped there and tried to make my video-RNN parameters trainable. But to do what? It was pretty clear I wouldn't make much headway on synthesis of natural video in two weeks, without experience in deep learning software frameworks, and without even a GPU to run on. I wanted a toy-sized problem which might still result in a cool interactive video process. So I came up with a different approach: rather than try to train a recurrent network I would train a feedforward convolutional network, then transplant its parameters into a still partially hand-constructed video process. I came up with a neat way to do that: my CNN would be arranged as an autoencoder. It would have an hourglass shape, moving information out of 2-D image space and into a dense vector representation (which I hand-wavingly hoped would make the network implement a "hierarchy of abstraction"). This would mean that I could bolt an "abstraction dimension" onto the temporal and spatial dimensions of a video feedback process. The autoencoder would implement "texture sampling" from the "less abstract" layer below and "more abstract" layer above. Then I could fiddle with the dynamics by implementing something like "each layer approaches the previous time-step minus the layer above plus the layer below, squashed".&lt;/p&gt;
&lt;p&gt;I almost bit off more than I could chew for a seminar project: my approach demanded that I design and train my own neural network with caffe &lt;em&gt;and&lt;/em&gt; re-implement the forward pass with OpenGL &lt;em&gt;and&lt;/em&gt; spend time exploring the resultant dynamics. I was able to train my autoencoders on &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;CIFAR&lt;/a&gt; with some success, and I was able to make some singular boiling multicolored nonsense. But I didn't get the spectacular emergence of natural image qualities I hoped for.&lt;/p&gt;
&lt;p&gt;Here's the &lt;a href="https://github.com/victor-shepardson/feature-feedback"&gt;GitHub&lt;/a&gt;, which includes a &lt;a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/writeup.ipynb"&gt;technical writeup&lt;/a&gt;, a &lt;a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/presentation.ipynb"&gt;jupyter notebook&lt;/a&gt; with the autoencoder experiments in it, and the (probably very brittle) source code for an openFrameworks app which runs the process interactively, optionally with webcam input. It's based on early 2015 versions of caffe and openFrameworks. I may still try to get the openFrameworks app running again and capture some video, for posterity.&lt;/p&gt;
&lt;p&gt;A few months later deep dream came out. Deep dream does a similar thing: it iteratively alters an image using a pre-trained CNN to manifest natural image qualities. The trick to deep dream is that the mechanism is the same as training the network, only turned upside down to optimize inputs instead of parameters. Vanilla deep dream converges, but it's simple to make a &lt;a href="https://www.youtube.com/watch?v=IREsx-xWQ0g"&gt;dynamic version&lt;/a&gt; by incorporating infinite zoom or similar. It's funny how I had the right idea about moving information up and down a hierarchy of abstraction, but failed to realize that backpropagation could do exactly that!&lt;/p&gt;&lt;/div&gt;</description><category>autoencoder</category><category>caffe</category><category>deep learning</category><category>lorenzo torresani</category><category>machine learning</category><category>openFrameworks</category><category>recurrent</category><category>video feedback</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item></channel></rss>