<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>~/blog (Posts about pytorch)</title><link>https://victor-shepardson.github.io/blog/</link><description></description><atom:link href="https://victor-shepardson.github.io/blog/categories/pytorch.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 07 May 2018 01:05:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Reproducing Î±-GAN</title><link>https://victor-shepardson.github.io/blog/posts/reproducing-alpha-gan/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;!-- Generative Adversarial Networks or *GANs* are one the most exciting ideas to emerge from the deep learning boom.

Generally speaking, generation seems hard compared to classification. Think of images -- you can tell if an image has a bear in it. But drawing a convincing bear of your own is more difficult. To *classify* images as bear/non-bear is easy, to *generate* bears is hard. And indeed this is true for computers; feed a large neural network enough labeled images and it will be able to label new ones for you fairly well. But the best methods for generating original images will give you bizarre mutants or vague blurs.

GANs use two networks to transmute generation into classification. A *generator* (G) network turns random codes into images, and a *discriminator* (D) network classifies images as either real or generated. Each network provides an objective for the other -- G tries to fool D while the D learns from data to stay one step ahead. G and D can be any differentiable neural network, so G can be trained by backpropagation through D. In limited domains, this works [spooky good](https://www.youtube.com/watch?v=G06dEcZ-QTg&amp;t=1m25s).

Ideally, a GAN gives you a dense code space from which you can sample random points on the data manifold. You can even take any two points in the code space and interpolate them to morph between images. Theoretically, any real (non-generated) image should have a corresponding point in the code space, but how do we find that point?

This is what Rosca et al. tackle in [Variational Approaches for Auto-Encoding
Generative Adversarial Networks](https://arxiv.org/abs/1706.04987) --&gt;

&lt;p&gt;I implemented Rosca, Mihaela, et al. &lt;a href="https://arxiv.org/abs/1706.04987"&gt;"Variational Approaches for Auto-Encoding
Generative Adversarial Networks"&lt;/a&gt; using Pytorch. It's a modular implementation -- plug in any torch modules as encoder, generator, discriminator and code discriminator.&lt;/p&gt;
&lt;p&gt;On &lt;a href="https://github.com/victor-shepardson/alpha-GAN"&gt;GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>autoencoder</category><category>computer vision</category><category>deep learning</category><category>GAN</category><category>machine learning</category><category>pytorch</category><guid>https://victor-shepardson.github.io/blog/posts/reproducing-alpha-gan/</guid><pubDate>Sun, 06 May 2018 23:36:51 GMT</pubDate></item></channel></rss>