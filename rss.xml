<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>~/blog</title><link>https://victor-shepardson.github.io/blog/</link><description>&gt; infernal knotted dark humming sunlight</description><atom:link href="https://victor-shepardson.github.io/blog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Wed, 06 Mar 2024 22:24:17 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Pond Brain (2023)</title><link>https://victor-shepardson.github.io/blog/posts/pond-brain/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;In 2023 I worked on &lt;em&gt;Pond Brain&lt;/em&gt;, a pair of installations by &lt;a href="https://www.instagram.com/jennasutela/"&gt;Jenna Sutela&lt;/a&gt; for Copenhagen Contemporary and the &lt;a href="https://helsinkibiennaali.fi/en/artist/jenna-sutela/"&gt;Helsinki Biennial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I trained real-time neural synthesis models with recordings of marine mammals and water sounds and built them into a feedback ecology with the sounds of musical instruments and stars from the Kepler exoplanet survey. Microphones in the room spun the sounds of visitors into the web of human, whale and celestial body.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://evermade-helsinkibiennaali-phase2-website.s3.eu-north-1.amazonaws.com/wp-content/uploads/2023/06/07143954/Jenna-Sutela_Pond-Brain-2.jpg"&gt;&lt;/p&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/pond-brain/</guid><pubDate>Wed, 06 Mar 2024 05:00:00 GMT</pubDate></item><item><title>Intelligent Instruments Lab</title><link>https://victor-shepardson.github.io/blog/posts/intelligent-instruments/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;Since 2021 I'm a PhD researcher in the &lt;a href="https://iil.is/"&gt;Intelligent Instruments Lab&lt;/a&gt;.&lt;/p&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/intelligent-instruments/</guid><pubDate>Mon, 04 Mar 2024 05:00:00 GMT</pubDate></item><item><title>Synesthesia Music Visualizer Scenes</title><link>https://victor-shepardson.github.io/blog/posts/synthesthesia/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;&lt;a href="http://synesthesia.live/"&gt;Synesthesia&lt;/a&gt; is VJ software for making audio-reactive visuals. It draws from the &lt;a href="https://www.shadertoy.com/"&gt;Shadertoy&lt;/a&gt; community, where everything is made with GLSL fragment shaders, but includes an easy way to add audio and video inputs and build simple MIDI-mappable control interfaces. Basically, everything a VJ needs to actually perform live with some shaders. The folks building Synesthesia seem super respectful of free-sharing ethos of Shadertoy -- they reached out to shader programmers like me and comissioned new works for Synesthesia, while keeping the actual shaders always hackable by users within the application. I wound up making a few fun video feedback and cellular-automata based scenes, and recently was goaded into using some of them to mess up the live stream from an algorave concert at the AIMC conference: &lt;/p&gt;
&lt;!-- https://youtu.be/d0RMUqcbhmQ?t=8639 --&gt;

&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
        &lt;iframe width="921" height="726" src="https://www.youtube.com/embed/d0RMUqcbhmQ?start=8639?rel=0" title="AIMC 2023 Concert 3 – Algorave" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</description><category>audiovisual</category><category>feedback</category><category>glsl</category><category>shadertoy</category><category>sonic</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/synthesthesia/</guid><pubDate>Sun, 03 Mar 2024 05:00:00 GMT</pubDate></item><item><title>Deviant Chain (2019)</title><link>https://victor-shepardson.github.io/blog/posts/deviant-chain/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;In 2019 I worked on a concert installation called &lt;a href="https://stefanmaier.studio/deviant-chain-2019/"&gt;&lt;em&gt;Deviant Chain&lt;/em&gt;&lt;/a&gt; with Stefan Maier and Alan Segal. Stefan, a composer, and Alan, a filmmaker, created a sort of art haunted house in which the audience moved from room to room in a former bank vault, encountering a series of short videos and sound works with themes of transhumanism and machine alterity. One of these dealt with the development of language and the human voice, and for this I created neural text-to-speech software for Stefan to gather sound material from. &lt;/p&gt;
&lt;p&gt;At this time, the &lt;a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/"&gt;WaveNet demo&lt;/a&gt; was in the air. This was one of the first neural generative models to work well with raw audio, and the uncanny babbling of the unconditional speech model was an arresting artifact of the ever-weirder capabilities of statistical autoregressive models. It had a few problems though: no open source implementation, very expensive training process, and extremely slow inference process. After some research, I found an open implementation of Tacotron2 from NVIDIA, which was state of the art for text-to-speech at the time, and substituted a WaveGlow vocoder for the original WaveNet. It turned out to still be quite slow and expensive to train WaveGlow, but it could actually generate audio with close to real-time throughput, and including a full text-to-speech model added many dimensions of control. This was a tool you could iterate with. I found the TTS could be induced to babble by letting it run past the end of its predicted alignment the the text, at which point it would get confused and start jumping around. Finally, I trained a few models on voices from Mozilla's &lt;a href="https://commonvoice.mozilla.org/en"&gt;CommonVoice&lt;/a&gt; project to get a much wider range of voice sounds. I built in some extra features to the CLI and got the whole thing running on Stefan's computer using PyInstaller. &lt;/p&gt;
&lt;p&gt;I also developed a small glyph generator which was used in some of the videos -- this took in sounds, extracted features with some machine listening algorithms, and then mapped those features to a set of curves and strokes and stored them as 3D mesh files. 
I traveled to Oslo to install Deviant Chain, setting up the video players and cueing system. I was pretty happy with the results -- it was genuinely spooky working on it with just a few of us in the empty bank basement at night.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/431684120" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/deviant-chain/</guid><pubDate>Sat, 02 Mar 2024 05:00:00 GMT</pubDate></item><item><title>Reproducing α-GAN</title><link>https://victor-shepardson.github.io/blog/posts/reproducing-alpha-gan/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;I implemented Rosca, Mihaela, et al. &lt;a href="https://arxiv.org/abs/1706.04987"&gt;"Variational Approaches for Auto-Encoding
Generative Adversarial Networks"&lt;/a&gt; using Pytorch. It's a modular implementation -- plug in any torch modules as encoder, generator, discriminator and code discriminator.&lt;/p&gt;
&lt;p&gt;On &lt;a href="https://github.com/victor-shepardson/alpha-GAN"&gt;GitHub&lt;/a&gt;&lt;/p&gt;</description><category>autoencoder</category><category>computer vision</category><category>deep learning</category><category>GAN</category><category>machine learning</category><category>pytorch</category><guid>https://victor-shepardson.github.io/blog/posts/reproducing-alpha-gan/</guid><pubDate>Sun, 06 May 2018 04:00:00 GMT</pubDate></item><item><title>Bear Cult and HOFSTADTERPILLAR</title><link>https://victor-shepardson.github.io/blog/posts/bear-cult-and-hofstadterpillar/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;These are two animations I made for FS35 with Jodie Mack in winter 2015.&lt;/p&gt;
&lt;h3&gt;Bear Cult&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Bear Cult&lt;/em&gt; was inspired by a collection of Joseph Campbell's essays I found in a used book store, specifically one entitled "Renewal Myths and Rites of the Primitive Hunters and Planters". It's about the prehistoric origins of myth. In Campbell's telling, the purpose of myth is to conquer death. Campbell cites preserved arrangements of cave bear bones as evidence for the ancient roots of a bear-baiting ritual still practiced by indigenous peoples in northern Japan. In the bear-baiting ritual, a young bear is captured, raised in captivity, and then ritually tormented, killed and eaten. The bear is believed to contain the soul of a demigod which yearns to be released from its fleshy bear-prison. For Campbell, this is the coping mechanism of an animal which kills to survive but understands death. It's the hunter's moral justification for killing: death isn't terminal, killing is a kindness. Ritualized death is a point of contact with the numinous; the fear of death is transmuted to awe. I found the deep connection Campbell makes between this sacred feeling and human capacity for cruelty heartbreaking.&lt;/p&gt;
&lt;p&gt;I decided to depict a bear-baiting ritual. Not the specific cultural practice Campbell recounts, but the abstracted elements of a hypothetical prehistoric bear cult. The body of the bear would be made by hand with paper drawings and cutouts, while the soul would be made in software with &lt;a href="https://victor-shepardson.github.io/blog/posts/video-feedback"&gt;virtual video feedback&lt;/a&gt;. These material and digital components would interact via fluorescent orange paper which could be keyed out in video editing software. A possible alternate title: "Mind-Body Chromakey". Originally I had storyboarded a longer narrative including human hands and spears piercing the bear and the escape of its soul, but only part proved feasible in the time I had.&lt;/p&gt;
&lt;p&gt;I'm happy with how the bear soul material turned out. After shooting the paper animation, I exported just the key as a binary mask. As an input to the feedback process, the mask lets the roiling cloud-forms conform to the geometry of the bear eyes and nose-hole. Unfortunately the black-on-black paper was unwise; there's a lot of camera noise after adjusting the levels to make everything visible. A darker background of black felt or something might have worked better.&lt;/p&gt;
&lt;p&gt;This project taught a lesson about how sound influences the perception of time in film. What felt like a good visual rhythm when I was silently animating seems weirdly abbreviated with the soundtrack added. The sound was a bit rushed, but I like the general effect. It's some recordings of bear vocalizations and carcass-mangling sounds chopped up into phase-rhythms with the same methods I used for &lt;a href="https://victor-shepardson.github.io/blog/posts/siba"&gt;SIBA&lt;/a&gt;. I'd like to revisit this project someday.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/199104868" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3&gt;HOFSTADTERPILLAR&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;HOFSTADTERPILLAR&lt;/em&gt; is a many-looped negotiation between rules, materials and intuition, an effort to explore Hofstadter's idea of &lt;a href="https://en.wikipedia.org/wiki/Strange_loop"&gt;strange loop&lt;/a&gt; by drawing.&lt;/p&gt;
&lt;p&gt;Drawing is an act of alternating abstraction and reification, recognition and construction of forms among materials. I draw, I see what I have drawn, I draw more. Animating by hand can be seen as a sort of &lt;a href="https://victor-shepardson.github.io/blog/posts/video-feedback"&gt;image feedback loop&lt;/a&gt;; each frame is drawn with reference to previous frames, which are visible beneath transparent cels. I developed loose sets of rules to draw by, of the type "a silver line extends and curves more each frame". Other rules emerge from the materials; paint markers pool and smear when the next cel is placed on top. I used the "maximum cycle" technique of repeatedly drawing new material into the same set of frames to construct rich and evolving loops. The sound is another set of feedback loops, a Max doodle with chaotic noise generators stimulating banks of tuned filters.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/121756116" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</description><category>aftereffects</category><category>animation</category><category>bear cult</category><category>douglas hofstadter</category><category>feedback</category><category>hofstadterpillar</category><category>jodie mack</category><category>joseph campbell</category><category>max</category><category>sonic</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/bear-cult-and-hofstadterpillar/</guid><pubDate>Thu, 19 Apr 2018 04:00:00 GMT</pubDate></item><item><title>SIBA I|II|III</title><link>https://victor-shepardson.github.io/blog/posts/SIBA/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;In 2014 I spent a lot of time working through a Steve Reich fascination by making tape music. This piece was eventually presented at ICMC 2015. SIBA I|II|III stands for "Studies In Being Alive One Two and Three", which was a joke about the lack of purpose I felt at the time and also a genuine attempt to describe what I was groping toward, stylized to reflect the asemic repetitious sound-destruction therein. I don't know how to feel about it.&lt;/p&gt;
&lt;iframe width="500" height="600" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/97523789&amp;amp;color=%2399aadd&amp;amp;auto_play=false&amp;amp;hide_related=true&amp;amp;show_comments=false&amp;amp;show_user=true&amp;amp;show_reposts=false&amp;amp;show_teaser=true&amp;amp;visual=true"&gt;&lt;/iframe&gt;</description><category>max</category><category>music</category><category>phasing</category><category>sound</category><category>steve reich</category><guid>https://victor-shepardson.github.io/blog/posts/SIBA/</guid><pubDate>Sun, 15 Apr 2018 04:00:00 GMT</pubDate></item><item><title>Video Feedback</title><link>https://victor-shepardson.github.io/blog/posts/video-feedback/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;Douglas Hofstadter's &lt;a href="https://archive.org/details/GEBen_201404"&gt;Gödel, Escher, Bach&lt;/a&gt; contains about a hundred captivating ideas, and just one of them is the concept of video feedback: connect a video camera to a screen, and then point it at the screen. Weird stuff happens on the screen as light is projected though the air, captured by the camera and circled back to the screen, mutating each time. Zoom out to get a infinite hall of mirrors effect, zoom in to get kaleidoscopic pulsations, loosen the focus to get bubbling slime.&lt;/p&gt;
&lt;p&gt;During college I became interested in computer graphics. Particularly, the idea of &lt;em&gt;procedural graphics&lt;/em&gt;: producing images with only code, as opposed to virtually drawing (as with photoshop) or digitizing reality (as with a digital camera). Procedural graphics can translate some of the tantalizing infinites of mathematics into rich sensory experiences! &lt;a href="http://www.iquilezles.org/www/articles/warp/warp.htm"&gt;A brief article&lt;/a&gt; by Íñigo Quílez introduced me to the idea of turning a random number generator into an endless alien terrain. One popular way to do this is with &lt;em&gt;fragment shaders&lt;/em&gt;, specialized programs for doing pixel-by-pixel graphics with the GPUs found in most computers. For more than a few examples, see &lt;a href="https://www.shadertoy.com/"&gt;Shadertoy&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;You can put a fragment shader into feedback just like a video camera. Pixels go in, pixels come out, pixels go back in 60 times per second. The shader determines how colors mutate and interact with nearby colors each time. Here are a few of mine:&lt;/p&gt;
&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/4d2BRm?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;
&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lt2yz3?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;
&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lsBcRK?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Other things which resemble digital video feedback:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D Cellular automata like &lt;a href="http://golly.sourceforge.net/"&gt;Conway's game of life&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finite element physical simulations of fluids and &lt;a href="https://pmneila.github.io/jsexp/grayscott/"&gt;reaction-diffusion systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional neural network visualization techniques like &lt;a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"&gt;deep dream&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other people working with this kind of stuff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sabrinaratte.com/"&gt;Sabrina Ratté&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://alexanderdupuis.com/"&gt;Alex Dupuis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/cornusammonis"&gt;Cornus Ammonis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pixlpa.com/"&gt;Andrew Benson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/Flexi23"&gt;Felix Woitzel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I just can't stay away. See projects such as &lt;a href="https://victor-shepardson.github.io/blog/posts/abstract-concrete/"&gt;ABSTRACT/CONCRETE&lt;/a&gt;, &lt;a href="https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/"&gt;Video Synthesis With Convolutional Autoencoders&lt;/a&gt; and &lt;a href="https://victor-shepardson.github.io/blog/posts/cortical-sonification"&gt;Data Sonification Using a Cortical Representation of Sound&lt;/a&gt;.&lt;/p&gt;</description><category>feedback</category><category>generative</category><category>shader</category><category>video</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/video-feedback/</guid><pubDate>Sun, 23 Apr 2017 04:00:00 GMT</pubDate></item><item><title>ABSTRACT/CONCRETE</title><link>https://victor-shepardson.github.io/blog/posts/abstract-concrete/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/164777442" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</description><category>audiovisual</category><category>feedback</category><category>generative</category><category>openFrameworks</category><category>sonic</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/abstract-concrete/</guid><pubDate>Wed, 11 Jan 2017 05:00:00 GMT</pubDate></item><item><title>Bendy: Wavetable Automata in Max</title><link>https://victor-shepardson.github.io/blog/posts/bendy/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;In spring of 2015 I made a synthesizer called Bendy as a seminar project. A technical description and a Max/MSP implementation can be found on &lt;a href="https://github.com/victor-shepardson/bendy"&gt;GitHub&lt;/a&gt;. Here's what is sounds like:&lt;/p&gt;
&lt;iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/209732439&amp;amp;color=%2399aadd&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false"&gt;&lt;/iframe&gt;</description><category>feedback</category><category>generative</category><category>max</category><category>sonic</category><guid>https://victor-shepardson.github.io/blog/posts/bendy/</guid><pubDate>Mon, 09 Jan 2017 05:00:00 GMT</pubDate></item></channel></rss>