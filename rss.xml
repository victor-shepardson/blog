<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Victor's Blog</title><link>https://victor-shepardson.github.io/blog/</link><description>the personal blog of Victor Shepardson</description><atom:link href="https://victor-shepardson.github.io/blog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 09 Jan 2017 23:36:50 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Making Music With No-input Mixer, SuperCollider, and Tidal</title><link>https://victor-shepardson.github.io/blog/posts/mixer-supercollider-tidal-music/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;h3&gt;No-input mixer&lt;/h3&gt;
&lt;p&gt;Years ago I one of &lt;a href="http://usa.yamaha.com/products/live_sound/mixers/analog-mixers/mg102c/"&gt;these&lt;/a&gt; cheap mixers so I could record and amplify dorm room nonsense music sessions. But at Dartmouth I had access to better recording facilities and the mixer was gathering dust. Cool dude &lt;a href="https://charlossound.wordpress.com/"&gt;Carlos Dominguez&lt;/a&gt; had introduced me to the concept of "no-input" mixing. Normally a mixer is the middleman between an instrument or microphone and a loudspeaker or recording device. It alters and facilitates sounds, but doesn't make sound. No-input mixing is total misuse of the mixer: you plug the mixer back into itself, as its own sole input. The mixer self-oscillates and makes its own sounds. It's the same principle as an electric guitar feeding back, or plugging a bunch of effects pedals together in a loop, or how analog synthesizers work under the hood. What's funny and exciting about the no-input mixer is how rich and diverse it sounds given that it isn't supposed to sound at all. The whole point of a mixer is to have fine control over routing and EQ; even a smallish stereo mixer has endless possible configurations. And because the oscillations depend on a delicate balance of amplification, it is an enormously sensitive instrument. Playing it means moving a single knob &lt;em&gt;so slowly&lt;/em&gt; to find the precise edge of chaos between two sounds; sweeping it &lt;em&gt;so quickly&lt;/em&gt; to carve a tiny blip out of a squall; listening &lt;em&gt;so closely&lt;/em&gt; to know when it's about to blow up. You learn the feel of a particular mixer, but it's still new every time. For some masterful no-input mixing, check out &lt;a href="https://en.wikipedia.org/wiki/Toshimaru_Nakamura"&gt;Toshimaru Nakamura&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As rich and surprisingly large as the space of feedback mixer sounds is, it's pretty distinctly within noise-drone-ambient world. You can make a lovely drone, you can make a wall of noise, you can make little squawking sounds, you can float through space. But I found myself yearning to hold on to one sound while searching for its perfect complement, or to condense a minute's worth of exploration into a short pattern, or to build whole stacks of sounds and rapidly switch between them. One option would be to ravenously seek &lt;em&gt;more channels&lt;/em&gt;, but lacking a bigger mixer to abuse I turned to my old friend, computers. At first I just ran the mixer into Ableton. Simple stuff like grabbing a loop while continuing to tweak the mixer, or stacking a weird noisy tone into a weird noisy chord. Once the computer's in the loop, you can also expand the mixer's vocabulary by feeding processed sound back into the mixer. Slight &lt;a href="https://en.wikipedia.org/wiki/Single-sideband_modulation"&gt;frequency shifting&lt;/a&gt; of 0.1-10 Hz and reverb are particularly fertile. "Senseless Noise feat. Jeff" is an extremely goofy example of this setup, with me twirling the knobs and cool dude Jeff Mentch playfully tickling the default Ableton percussion synth. Also there are some blippy sine waves.&lt;/p&gt;
&lt;iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/247613308&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false"&gt;&lt;/iframe&gt;

&lt;h3&gt;Live Coding&lt;/h3&gt;
&lt;p&gt;Using a big nasty rigid-yet-complex GUI like Ableton with the no-input mixer felt all wrong; it's nice for recording and has a powerful sampler, but it really spoils the elegance of the no-input mixer. And it demands the extra moving part of a MIDI controller or at least a mouse. So lately I've been learning to use two live-coding music languages, &lt;a href="http://supercollider.github.io/"&gt;SuperCollider&lt;/a&gt; and &lt;a href="https://tidalcycles.org/"&gt;Tidal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SuperCollider is two things: a flexible software synthesizer &lt;em&gt;scsynth&lt;/em&gt; and a scripting language to control it, &lt;em&gt;sclang&lt;/em&gt;. (Really three, there's also the development environment &lt;em&gt;scide&lt;/em&gt;). In SuperCollider, you build synthesizers as graphs of unit generators which do simple things like generate a sine wave, delay a signal, apply a filter. Sclang lets you do things which are tedious in a program like Max or Ableton and impossible with hardware, like "make 100 versions of this sound and spread them across the stereo field" or "randomize all the connections between these synthesizers". It does come with a steep learning curve. Sclang has a funny syntax with some gotchas like "literals can't start with &lt;code&gt;.&lt;/code&gt;" and "single letter names are special global variables" and "functions are first class but you have to call them by accessing their &lt;code&gt;value&lt;/code&gt; field or there's a special shorthand &lt;code&gt;f.(args)&lt;/code&gt; which just looks weird". There are always several programming paradigms and syntax options to choose from. The separation between scsynth and sclang takes a while to wrap your head around. Scsynth and scide can be temperamental. Errors aren't very informative. It's enough to drive you off when you're starting out and just want to make a weird chord out of 100 sine waves. Nevertheless, I've been getting the hang of it and having a blast dreaming up wacky delay effects to use with the mixer.&lt;/p&gt;
&lt;p&gt;Tidal is complementary to SuperCollider. It doesn't deal directly with sound, but focuses on music as a sequence of discrete events. Like Ableton, it enshrines pulse as fundamental. Very unlike Ableton, it brings all the machinery of maximally-dorky pure functional programming to bear on the process of defining musical patterns. Tidal is two things: a sublanguage of &lt;a href="https://www.haskell.org/"&gt;Haskell&lt;/a&gt; with its own special "pattern" literal, and a runtime which maintains a clock and emits OSC messages. I found it easy to jump in, but only because I already spent some time quality time with Haskell and functional programming in a PL class. It's abstract as hell. &lt;a href="http://learnyouahaskell.com/"&gt;This silly book&lt;/a&gt; is a good introduction. Tidal tries pretty hard to be friendly, but until you understand what the &lt;code&gt;$&lt;/code&gt; operator means you will have a bad time. Tidal can send arbitrary MIDI and OSC, but it's really made to use with its own special sampler called Dirt (which has a SuperCollider implementation). I've been recording trajectories of a few minutes through computer-augmented no-input mixer space, then using Tidal+Dirt to chop them up into patterns. "Improv December 17" is all mixer plus a single kick drum sample that comes with Dirt.&lt;/p&gt;
&lt;iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/298537508&amp;amp;color=ff5500&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false"&gt;&lt;/iframe&gt;

&lt;p&gt;Tidal and Dirt are pretty experimental software, and there are some rough edges. Not all the documentation is there, and some things just aren't working for me. Sometimes samples don't fade in right and pop, the built in delay effect is acting bizarrely, and it can be awkward to deal with long samples. Right off the bat, I had to build the latest version to fix a fatal bug. There are some sampler features I miss like having expressive envelopes; I'd also like more flexible EQ. If I can get fluent at SuperCollider, I may try to implement some of it myself!&lt;/p&gt;
&lt;p&gt;So far all my SuperCollider and Tidal work is live-coded spaghetti, but when I have some real time to dedicate I hope to pack some of it into nice libraries. Stay tuned!&lt;/p&gt;
&lt;p&gt;Side note: I nuked my old Windows desktop to turn it into an Ubuntu server/workstation for machine learning stuff. Tidal and SuperCollider are cross platform, but I normally use Ableton or Reaper for a DAW, which don't run under Linux. So I tried out &lt;a href="https://ardour.org/"&gt;Ardour&lt;/a&gt; for the first time in years. I was very impressed until I discovered that automation curves wouldn't draw right. So close! If I can get to the bottom of that, I'll have little reason to leave Linux again. Since I hadn't set up multichannel recording for Dirt and SuperCollider yet, I needed to aggressively master a dry stereo recording. I tried out the free &lt;a href="http://calf-studio-gear.org/"&gt;Calf plugins&lt;/a&gt;, which are very flexible and sound great. I guess they've been around for a while, but I never knew about them before.&lt;/p&gt;&lt;/div&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/mixer-supercollider-tidal-music/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Data Sonification Using a Cortical Representation of Sound</title><link>https://victor-shepardson.github.io/blog/posts/cortical-sonification/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;TODO&lt;/p&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/cortical-sonification/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Video Synthesis With Convolutional Autoencoders</title><link>https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;h2&gt;TODO&lt;/h2&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Bendy: Wavetable Automata in Max</title><link>https://victor-shepardson.github.io/blog/posts/bendy/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;h2&gt;TODO&lt;/h2&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/bendy/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Hello!</title><link>https://victor-shepardson.github.io/blog/posts/hello/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;For 2017 I resolve to &lt;strong&gt;document more&lt;/strong&gt;. I'll be reflecting on old projects here and posting about current ones.&lt;/p&gt;
&lt;p&gt;Rather than a chronological feed of posts, I plan to have one per project which I'll update as each one progresses. Stay tuned.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://victor-shepardson.github.io/blog/posts/hello/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://victor-shepardson.github.io/blog/posts/hello/</guid><pubDate>Fri, 06 Jan 2017 22:40:25 GMT</pubDate></item></channel></rss>