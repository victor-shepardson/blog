<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>~/blog</title><link>https://victor-shepardson.github.io/blog/</link><description>&gt; infernal knotted dark humming sunlight</description><atom:link href="https://victor-shepardson.github.io/blog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 07 May 2018 01:35:56 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Reproducing Î±-GAN</title><link>https://victor-shepardson.github.io/blog/posts/reproducing-alpha-gan/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;!-- Generative Adversarial Networks or *GANs* are one the most exciting ideas to emerge from the deep learning boom.

Generally speaking, generation seems hard compared to classification. Think of images -- you can tell if an image has a bear in it. But drawing a convincing bear of your own is more difficult. To *classify* images as bear/non-bear is easy, to *generate* bears is hard. And indeed this is true for computers; feed a large neural network enough labeled images and it will be able to label new ones for you fairly well. But the best methods for generating original images will give you bizarre mutants or vague blurs.

GANs use two networks to transmute generation into classification. A *generator* (G) network turns random codes into images, and a *discriminator* (D) network classifies images as either real or generated. Each network provides an objective for the other -- G tries to fool D while the D learns from data to stay one step ahead. G and D can be any differentiable neural network, so G can be trained by backpropagation through D. In limited domains, this works [spooky good](https://www.youtube.com/watch?v=G06dEcZ-QTg&amp;t=1m25s).

Ideally, a GAN gives you a dense code space from which you can sample random points on the data manifold. You can even take any two points in the code space and interpolate them to morph between images. Theoretically, any real (non-generated) image should have a corresponding point in the code space, but how do we find that point?

This is what Rosca et al. tackle in [Variational Approaches for Auto-Encoding
Generative Adversarial Networks](https://arxiv.org/abs/1706.04987) --&gt;

&lt;p&gt;I implemented Rosca, Mihaela, et al. &lt;a href="https://arxiv.org/abs/1706.04987"&gt;"Variational Approaches for Auto-Encoding
Generative Adversarial Networks"&lt;/a&gt; using Pytorch. It's a modular implementation -- plug in any torch modules as encoder, generator, discriminator and code discriminator.&lt;/p&gt;
&lt;p&gt;On &lt;a href="https://github.com/victor-shepardson/alpha-GAN"&gt;GitHub&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>autoencoder</category><category>computer vision</category><category>deep learning</category><category>GAN</category><category>machine learning</category><category>pytorch</category><guid>https://victor-shepardson.github.io/blog/posts/reproducing-alpha-gan/</guid><pubDate>Sun, 06 May 2018 23:36:51 GMT</pubDate></item><item><title>Bear Cult and HOFSTADTERPILLAR</title><link>https://victor-shepardson.github.io/blog/posts/bear-cult-and-hofstadterpillar/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;These are two animations I made for FS35 with Jodie Mack in winter 2015.&lt;/p&gt;
&lt;h3&gt;Bear Cult&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Bear Cult&lt;/em&gt; was inspired by a collection of Joseph Campbell's essays I found in a used book store, specifically one entitled "Renewal Myths and Rites of the Primitive Hunters and Planters". It's about the prehistoric origins of myth. In Campbell's telling, the purpose of myth is to conquer death. Campbell cites preserved arrangements of cave bear bones as evidence for the ancient roots of a bear-baiting ritual still practiced by indigenous peoples in northern Japan. In the bear-baiting ritual, a young bear is captured, raised in captivity, and then ritually tormented, killed and eaten. The bear is believed to contain the soul of a demigod which yearns to be released from its fleshy bear-prison. For Campbell, this is the coping mechanism of an animal which kills to survive but understands death. It's the hunter's moral justification for killing: death isn't terminal, killing is a kindness. Ritualized death is a point of contact with the numinous; the fear of death is transmuted to awe. I found the deep connection Campbell makes between this sacred feeling and human capacity for cruelty heartbreaking.&lt;/p&gt;
&lt;p&gt;I decided to depict a bear-baiting ritual. Not the specific cultural practice Campbell recounts, but the abstracted elements of a hypothetical prehistoric bear cult. The body of the bear would be made by hand with paper drawings and cutouts, while the soul would be made in software with &lt;a href="https://victor-shepardson.github.io/blog/posts/video-feedback"&gt;virtual video feedback&lt;/a&gt;. These material and digital components would interact via fluorescent orange paper which could be keyed out in video editing software. A possible alternate title: "Mind-Body Chromakey". Originally I had storyboarded a longer narrative including human hands and spears piercing the bear and the escape of its soul, but only part proved feasible in the time I had.&lt;/p&gt;
&lt;p&gt;I'm happy with how the bear soul material turned out. After shooting the paper animation, I exported just the key as a binary mask. As an input to the feedback process, the mask lets the roiling cloud-forms conform to the geometry of the bear eyes and nose-hole. Unfortunately the black-on-black paper was unwise; there's a lot of camera noise after adjusting the levels to make everything visible. A darker background of black felt or something might have worked better.&lt;/p&gt;
&lt;p&gt;This project taught a lesson about how sound influences the perception of time in film. What felt like a good visual rhythm when I was silently animating seems weirdly abbreviated with the soundtrack added. The sound was a bit rushed, but I like the general effect. It's some recordings of bear vocalizations and carcass-mangling sounds chopped up into phase-rhythms with the same methods I used for &lt;a href="https://victor-shepardson.github.io/blog/posts/siba"&gt;SIBA&lt;/a&gt;. I'd like to revisit this project someday.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/199104868" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3&gt;HOFSTADTERPILLAR&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;HOFSTADTERPILLAR&lt;/em&gt; is a many-looped negotiation between rules, materials and intuition, an effort to explore Hofstadter's idea of &lt;a href="https://en.wikipedia.org/wiki/Strange_loop"&gt;strange loop&lt;/a&gt; by drawing.&lt;/p&gt;
&lt;p&gt;Drawing is an act of alternating abstraction and reification, recognition and construction of forms among materials. I draw, I see what I have drawn, I draw more. Animating by hand can be seen as a sort of &lt;a href="https://victor-shepardson.github.io/blog/posts/video-feedback"&gt;image feedback loop&lt;/a&gt;; each frame is drawn with reference to previous frames, which are visible beneath transparent cels. I developed loose sets of rules to draw by, of the type "a silver line extends and curves more each frame". Other rules emerge from the materials; paint markers pool and smear when the next cel is placed on top. I used the "maximum cycle" technique of repeatedly drawing new material into the same set of frames to construct rich and evolving loops. The sound is another set of feedback loops, a Max doodle with chaotic noise generators stimulating banks of tuned filters.&lt;/p&gt;
&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/121756116" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>aftereffects</category><category>animation</category><category>bear cult</category><category>douglas hofstadter</category><category>feedback</category><category>hofstadterpillar</category><category>jodie mack</category><category>joseph campbell</category><category>max</category><category>sonic</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/bear-cult-and-hofstadterpillar/</guid><pubDate>Thu, 19 Apr 2018 04:00:00 GMT</pubDate></item><item><title>SIBA I|II|III</title><link>https://victor-shepardson.github.io/blog/posts/SIBA/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;In 2014 I spent a lot of time working through a Steve Reich obsession by making tape music. This piece was eventually presented at ICMC 2015. SIBA I|II|III stands for "Studies In Being Alive One Two and Three", which was a joke about the lack of purpose I felt at the time and also a genuine attempt to describe what I was groping toward, stylized to reflect the asemic repetitious sound-destruction therein. I don't know how to feel about it.&lt;/p&gt;
&lt;iframe width="500" height="600" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/97523789&amp;amp;color=%2399aadd&amp;amp;auto_play=false&amp;amp;hide_related=true&amp;amp;show_comments=false&amp;amp;show_user=true&amp;amp;show_reposts=false&amp;amp;show_teaser=true&amp;amp;visual=true"&gt;&lt;/iframe&gt;&lt;/div&gt;</description><category>max</category><category>music</category><category>phasing</category><category>sound</category><category>steve reich</category><guid>https://victor-shepardson.github.io/blog/posts/SIBA/</guid><pubDate>Sun, 15 Apr 2018 04:00:00 GMT</pubDate></item><item><title>Video Feedback</title><link>https://victor-shepardson.github.io/blog/posts/video-feedback/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;Douglas Hofstadter's &lt;a href="https://archive.org/details/GEBen_201404"&gt;GÃ¶del, Escher, Bach&lt;/a&gt; contains about a hundred captivating ideas, and just one of them is the concept of video feedback: connect a video camera to a screen, and then point it at the screen. Weird stuff happens on the screen as light is projected though the air, captured by the camera and circled back to the screen, mutating each time. Zoom out to get a infinite hall of mirrors effect, zoom in to get kaleidoscopic pulsations, loosen the focus to get bubbling slime.&lt;/p&gt;
&lt;p&gt;During college I became interested in computer graphics. Particularly, the idea of &lt;em&gt;procedural graphics&lt;/em&gt;: producing images with only code, as opposed to virtually drawing (as with photoshop) or digitizing reality (as with a digital camera). Procedural graphics can translate some of the tantalizing infinites of mathematics into rich sensory experiences! &lt;a href="http://www.iquilezles.org/www/articles/warp/warp.htm"&gt;A brief article&lt;/a&gt; by ÃÃ±igo QuÃ­lez introduced me to the idea of turning a random number generator into an endless alien terrain. One popular way to do this is with &lt;em&gt;fragment shaders&lt;/em&gt;, specialized programs for doing pixel-by-pixel graphics with the GPUs found in most computers. For more than a few examples, see &lt;a href="https://www.shadertoy.com/"&gt;Shadertoy&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;You can put a fragment shader into feedback just like a video camera. Pixels go in, pixels come out, pixels go back in 60 times per second. The shader determines how colors mutate and interact with nearby colors each time. Here are a few of mine:&lt;/p&gt;
&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/4d2BRm?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;

&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lt2yz3?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;

&lt;iframe width="100%" height="256" frameborder="0" src="https://www.shadertoy.com/embed/lsBcRK?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Other things which resemble digital video feedback:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D Cellular automata like &lt;a href="http://golly.sourceforge.net/"&gt;Conway's game of life&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finite element physical simulations of fluids and &lt;a href="https://pmneila.github.io/jsexp/grayscott/"&gt;reaction-diffusion systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional neural network visualization techniques like &lt;a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"&gt;deep dream&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other people working with this kind of stuff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sabrinaratte.com/"&gt;Sabrina RattÃ©&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://alexanderdupuis.com/"&gt;Alex Dupuis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/cornusammonis"&gt;Cornus Ammonis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pixlpa.com/"&gt;Andrew Benson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/Flexi23"&gt;Felix Woitzel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I just can't stay away. See projects such as &lt;a href="https://victor-shepardson.github.io/blog/posts/abstract-concrete/"&gt;ABSTRACT/CONCRETE&lt;/a&gt;, &lt;a href="https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/"&gt;Video Synthesis With Convolutional Autoencoders&lt;/a&gt; and &lt;a href="https://victor-shepardson.github.io/blog/posts/cortical-sonification"&gt;Data Sonification Using a Cortical Representation of Sound&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>feedback</category><category>generative</category><category>shader</category><category>video</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/video-feedback/</guid><pubDate>Sun, 23 Apr 2017 04:00:00 GMT</pubDate></item><item><title>ABSTRACT/CONCRETE</title><link>https://victor-shepardson.github.io/blog/posts/abstract-concrete/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;!--
.. title: ABSTRACT/CONCRETE
.. slug: abstract-concrete
.. date: 2017-01-11 22:05:57 UTC-05:00
.. tags: sonic, visual, audiovisual, feedback, generative, openFrameworks
.. category:
.. link:
.. description:
.. type: text
--&gt;

&lt;style&gt;
  .video-wrapper {
   width: 100%;
   display: inline-block;
   position: relative;
  }
  .video-wrapper:after {
      padding-top: 56.25%; /*16:9 ratio*/
      display: block;
      content: '';
  }
  .video {
      position: absolute;
      top: 0; bottom: 0; right: 0; left: 0;
  }
&lt;/style&gt;

&lt;div class="video-wrapper"&gt;
  &lt;div class="video"&gt;
    &lt;iframe src="https://player.vimeo.com/video/164777442" width="100%" height="100%" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>audiovisual</category><category>feedback</category><category>generative</category><category>openFrameworks</category><category>sonic</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/abstract-concrete/</guid><pubDate>Thu, 12 Jan 2017 03:05:57 GMT</pubDate></item><item><title>Bendy: Wavetable Automata in Max</title><link>https://victor-shepardson.github.io/blog/posts/bendy/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;In spring of 2015 I made a synthesizer called Bendy as a seminar project. A technical description and a Max/MSP implementation can be found on &lt;a href="https://github.com/victor-shepardson/bendy"&gt;GitHub&lt;/a&gt;. Here's what is sounds like:&lt;/p&gt;
&lt;iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/209732439&amp;amp;color=%2399aadd&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false"&gt;&lt;/iframe&gt;&lt;/div&gt;</description><category>feedback</category><category>generative</category><category>max</category><category>sonic</category><guid>https://victor-shepardson.github.io/blog/posts/bendy/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Data Sonification Using a Cortical Representation of Sound</title><link>https://victor-shepardson.github.io/blog/posts/cortical-sonification/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;p&gt;&lt;a href="https://github.com/victor-shepardson/video-feedback-sonification"&gt;GitHub&lt;/a&gt;&lt;/p&gt;</description><category>audiovisual</category><category>matlab</category><category>michael casey</category><category>neuroscience</category><category>sonic</category><category>sonification</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/cortical-sonification/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Making Music With No-input Mixer, SuperCollider, and Tidal</title><link>https://victor-shepardson.github.io/blog/posts/mixer-supercollider-tidal-music/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;h3&gt;No-input Mixer&lt;/h3&gt;
&lt;p&gt;Years ago I got one of &lt;a href="http://usa.yamaha.com/products/live_sound/mixers/analog-mixers/mg102c/"&gt;these&lt;/a&gt; cheap mixers so I could record and amplify dorm room nonsense music sessions. But in grad school I had access to better recording facilities and the mixer was gathering dust. Also at that time, cool dude &lt;a href="https://charlossound.wordpress.com/"&gt;Carlos Dominguez&lt;/a&gt; introduced me to the concept of "no-input" mixing. Normally a mixer is the middleman between an instrument or microphone and a loudspeaker or recording device. It alters and facilitates sounds, but doesn't make sound. No-input mixing is total misuse of the mixer: you plug the mixer back into itself, as its own sole input. The mixer self-oscillates and makes its own sounds. It's the same principle as an electric guitar or microphone feeding back. What's funny and exciting about the no-input mixer is how rich and diverse it sounds given that it isn't supposed to sound at all. The whole point of a mixer is to have fine control over sound routing and equalization, so even a smallish stereo mixer has endless possible configurations. And because the oscillations depend on a delicate balance of amplification, no-input mixer is an enormously sensitive instrument. Playing one means moving a single knob &lt;em&gt;so slowly&lt;/em&gt; to find the precise edge of chaos between two sounds; sweeping it &lt;em&gt;so quickly&lt;/em&gt; to carve a tiny blip out of a squall; listening &lt;em&gt;so closely&lt;/em&gt; to know when it's about to blow up. You learn the feel of a particular mixer, but it's still new every time. For some masterful no-input mixing, check out &lt;a href="https://en.wikipedia.org/wiki/Toshimaru_Nakamura"&gt;Toshimaru Nakamura&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As rich and surprisingly large as the space of feedback mixer sounds is, it's pretty distinctly within noise-drone-ambient world. You can make a lovely drone, you can make a wall of noise, you can make little squawking sounds, you can float through space. But I found myself yearning to hold on to one sound while searching for its perfect complement, or to condense a minute's worth of exploration into a short pattern, or to build whole stacks of sounds and rapidly switch between them. One option would be to ravenously &lt;em&gt;seek more channels&lt;/em&gt;, but lacking a bigger mixer to abuse I turned to my old friend, computers. At first I just ran the mixer into Ableton. Simple stuff like grabbing a loop while continuing to tweak the mixer, or stacking a weird noisy tone into a weird noisy chord. Once the computer's in the loop, you can also expand the mixer's vocabulary by feeding processed sound back into the mixer. Slight &lt;a href="https://en.wikipedia.org/wiki/Single-sideband_modulation"&gt;frequency shifting&lt;/a&gt; of 0.1-10 Hz and reverb are particularly fertile. "Senseless Noise feat. Jeff" is an extremely goofy example of this setup, with me twirling the knobs and cool dude Jeff Mentch playfully tickling the default Ableton percussion synth. Also there are some blippy sine waves.&lt;/p&gt;
&lt;iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/247613308&amp;amp;color=%2399aadd&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false"&gt;&lt;/iframe&gt;

&lt;h3&gt;Live Coding&lt;/h3&gt;
&lt;p&gt;Using a big nasty rigid-yet-complex GUI like Ableton with the no-input mixer felt all wrong; it's nice for recording and has a powerful sampler, but it really spoils the elegance of the no-input mixer. And it demands the extra moving part of a MIDI controller or at least a mouse. As an alternative I've finally been getting into live coding. The usual way of programming involves writing code in a text file, and then running it all at once. Maybe the program halts and you inspect the result, or maybe it runs in a loop doing something until you close it. Only then can you can alter the code and try again. This is not an ideal paradigm for making music with code. A better one would be to have the program running as you write and revise it bit by bit, and have each change reflected in what you hear. Then instead of trying to program music ahead of time, the programming &lt;em&gt;is&lt;/em&gt; the music. That's the idea of live coding. Some &lt;a href="https://toplap.org/about/"&gt;exponents of live coding&lt;/a&gt; are really into the performance angle, putting the programmer on a stage and projecting code for an audience to see.&lt;/p&gt;
&lt;p&gt;Lately I've been learning to use two live-coding music tools, &lt;a href="http://supercollider.github.io/"&gt;SuperCollider&lt;/a&gt; for audio and &lt;a href="https://tidalcycles.org/"&gt;Tidal&lt;/a&gt; for patterns.&lt;/p&gt;
&lt;p&gt;SuperCollider is three things: a flexible software synthesizer &lt;em&gt;scsynth&lt;/em&gt;, a scripting language &lt;em&gt;sclang&lt;/em&gt; to control it, and a development environment &lt;em&gt;scide&lt;/em&gt;. With SuperCollider, you build synthesizers as graphs of unit generators which do simple things like generate a sine wave, delay a signal, apply a filter. Sclang can compactly do things which are tedious in a program like Max or Ableton and impossible with hardware, like "make 100 versions of this sound and spread them across the stereo field" or "randomize all the connections between these synthesizers". It does come with a steep learning curve. There are always several programming paradigms and syntax options to choose from; the separation between scsynth and sclang takes a while to wrap your head around; scsynth and scide can be temperamental; errors can be uninformative. It's enough to drive you off when you're starting out and just want to make a weird chord out of 100 sine waves. Nevertheless, I've been getting the hang of it and having a blast dreaming up wacky delay effects to use with the mixer.&lt;/p&gt;
&lt;p&gt;Tidal is complementary to SuperCollider. It doesn't deal directly with sound, but focuses on music as a sequence of discrete events. Like Ableton Live, it enshrines pulse: everything is a loop. Very unlike Ableton, it brings all the machinery of maximally-dorky pure functional programming to bear on the definition of musical patterns. Tidal is two things: a sublanguage of &lt;a href="https://www.haskell.org/"&gt;Haskell&lt;/a&gt;, and a runtime which maintains a clock and emits OSC messages. With Tidal, you write a simple pattern as something like &lt;code&gt;sound "bd sn bd sn"&lt;/code&gt; meaning "kick, snare, kick, snare". The part in quotes isn't a string, but Tidal's special pattern literal. You construct different rhythms using nesting and rests, e.g. &lt;code&gt;"hh [hh [~ hh]]"&lt;/code&gt;. Immediately, I liked this much better than clicking around a piano roll (though it's obviously very different from playing on a controller). You can then pattern other parameters by adding something like &lt;code&gt;# pan "0 1" # gain "0.1 0.2 0.3"&lt;/code&gt;. Each pattern can have its own meter. You can transform a pattern by e.g. changing the speed; you can superimpose or concatenate lists of patterns. And since Tidal is a &lt;em&gt;language&lt;/em&gt;, you can do all that recursively, building complex patterns up of simple atoms, inventing and reusing processes. Tidal can send arbitrary MIDI and OSC, but it's really made to use with its own special sampler called Dirt (which has a SuperCollider implementation). I've been recording trajectories of a few minutes through computer-augmented no-input mixer space, then using Tidal+Dirt to chop them up into patterns. "Improv December 17" is made from one long mixer recording and a bass drum sample that comes with Dirt.&lt;/p&gt;
&lt;iframe width="100%" height="166" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/298537508&amp;amp;color=%2399aadd&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false"&gt;&lt;/iframe&gt;

&lt;p&gt;Using Tidal is a lot like programming in Haskell, because that's what it is. It feels natural to work with lists and higher order functions, but deeply weird when you want to do something in an imperative style. Haskell is sort of an odd match for live coding. In the middle of a groove, the last thing I want to worry about is whether to use &lt;code&gt;fromRational&lt;/code&gt; or &lt;code&gt;floor&lt;/code&gt; to make sure some number is the right type to play nice with a Tidal function. On the other hand, Haskell and the ingenious pattern literal make for an incredibly concise syntax. And though Haskell's strong type safety can add cognitive load I'd rather spend on musical matters, it also makes it harder to break: a mistake is more likely to be rejected by the interpreter (doing nothing) than to do something unexpected to the sound.&lt;/p&gt;
&lt;p&gt;Tidal and SuperDirt are pretty experimental software, and there are some rough edges. Not all the documentation is there, and some things just aren't working for me. Sometimes samples don't fade in right and pop, the built in delay effect is acting bizarrely, and it can be awkward to deal with long samples. Right off the bat, I had to build the latest version to fix a fatal bug. There are some sampler features I miss like having expressive envelopes; I'd also like more flexible EQ. If I can get fluent at SuperCollider, I may try to implement some of these things myself.&lt;/p&gt;
&lt;p&gt;So far all my SuperCollider and Tidal work is live-coded spaghetti, but eventually I hope to pack some of it into nice libraries. Stay tuned!&lt;/p&gt;
&lt;p&gt;Side note: lately I've been using an Ubuntu desktop for music. To record, edit and master material coming out of SuperCollider, I tried out &lt;a href="https://ardour.org/"&gt;Ardour&lt;/a&gt; for the first time in years. I was impressed by everything until I discovered that automation curves wouldn't draw right. So close! I also tried out the free &lt;a href="http://calf-studio-gear.org/"&gt;Calf plugins&lt;/a&gt;, which are very flexible and sound great. Seems they've been around for a while, but I never knew about them before. The multiband compressor and gate effects worked well for massaging the sound of a dense stereo recording.&lt;/p&gt;&lt;/div&gt;</description><category>feedback</category><category>music</category><category>sonic</category><category>supercollider</category><category>tidal</category><guid>https://victor-shepardson.github.io/blog/posts/mixer-supercollider-tidal-music/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item><item><title>Video Synthesis With Convolutional Autoencoders</title><link>https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/</link><dc:creator>Victor Shepardson</dc:creator><description>&lt;div&gt;&lt;p&gt;This project was my attempt to incorporate a neural network trained to encode images into a &lt;a href="https://victor-shepardson.github.io/blog/posts/video-feedback"&gt;video feedback process&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In spring of 2015 I took a &lt;a href="http://www.cs.dartmouth.edu/~lorenzo/teaching/cs189/Archive/Spring2015/"&gt;seminar in deep learning&lt;/a&gt; which got me real excited. Machine learning is the study of general methods for problem solving using optimization and data. Deep learning is a particular approach to ML using models with many differentiable layers of parameters. Especially interesting to me was &lt;em&gt;representation learning&lt;/em&gt;: using ML methods to extract meaningful features from "raw" data like the pixels of an image. And I'd been talking a lot to &lt;a href="http://pkmital.com/home/"&gt;Parag Mital&lt;/a&gt; and &lt;a href="http://www.cs.dartmouth.edu/~sarroff/"&gt;Andy Sarroff&lt;/a&gt; about their respective work with deep learning, sound and video. But what freaked me out the most about deep learning was the similarity between neural networks and the audio/video feedback I'd been using to make noise.&lt;/p&gt;
&lt;p&gt;The kind of digital video feedback I'd been playing with was superficially like a recurrent neural network. At each time step, the current frame of video would be computed from the last (and optionally, the current frame of an input video). There would first be some linear function from images to images, like translation or blurring; generally, each pixel would take on a linear combination of pixels in the last frame and input frame. Then, there would be some pixel-wise bounded nonlinearity to keep the process from blowing up, like wrapping around [0, 1] or sigmoid squashing. That's the architecture of an RNN. The only difference was that rather than represent the linear transformation as a big ol' parameter matrix, I would hand-craft it from a few sampling operations in a fragment shader. And instead of training by backpropagation to do some task, I would fiddle with it manually until it had visually interesting dynamics.&lt;/p&gt;
&lt;p&gt;I might have stopped there and tried to make my video-RNN parameters trainable. But to do what? It was pretty clear I wouldn't make much headway on synthesis of natural video in two weeks, without experience in deep learning software frameworks, and without even a GPU to run on. I wanted a toy-sized problem which might still result in a cool interactive video process. So I came up with a different approach: rather than try to train a recurrent network I would train a feedforward convolutional network, then transplant its parameters into a still partially hand-constructed video process. I came up with a neat way to do that: my CNN would be arranged as an autoencoder. It would have an hourglass shape, moving information out of 2-D image space and into a dense vector representation (which I vaguely hoped would make the network implement a "hierarchy of abstraction"). This would mean that I could bolt an "abstraction dimension" onto the temporal and spatial dimensions of a video feedback process. The autoencoder would implement "texture sampling" from the "less abstract" layer below and "more abstract" layer above. Then I could fiddle with the dynamics by implementing something like "each layer approaches the previous time-step minus the layer above plus the layer below, squashed".&lt;/p&gt;
&lt;p&gt;I almost bit off more than I could chew for a seminar project: my approach demanded that I design and train my own neural network with caffe &lt;em&gt;and&lt;/em&gt; re-implement the forward pass with OpenGL &lt;em&gt;and&lt;/em&gt; spend time exploring the resultant dynamics. I was able to train my autoencoders on &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;CIFAR&lt;/a&gt; with some success, and I was able to make some singular boiling multicolored nonsense. But I didn't get the spectacular emergence of natural image qualities I hoped for.&lt;/p&gt;
&lt;p&gt;Here's the &lt;a href="https://github.com/victor-shepardson/feature-feedback"&gt;GitHub&lt;/a&gt;, which includes a &lt;a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/writeup.ipynb"&gt;technical writeup&lt;/a&gt;, a &lt;a href="https://github.com/victor-shepardson/feature-feedback/blob/master/notebooks/presentation.ipynb"&gt;jupyter notebook&lt;/a&gt; with the autoencoder experiments in it, and the (probably very brittle) source code for an openFrameworks app which runs the process interactively, optionally with webcam input. It's based on early 2015 versions of caffe and openFrameworks. I may still try to get the openFrameworks app running again and capture some video, for posterity.&lt;/p&gt;
&lt;p&gt;A few months later &lt;a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"&gt;deep dream&lt;/a&gt; came out. Deep dream does a similar thing: it iteratively alters an image using a pre-trained CNN to manifest natural image qualities. The trick to deep dream is that the mechanism is the same as training the network, optimizing inputs instead of parameters. Vanilla deep dream converges, but it's simple to make a &lt;a href="https://www.youtube.com/watch?v=IREsx-xWQ0g"&gt;dynamic version&lt;/a&gt; by incorporating infinite zoom or similar. Too bad I didn't get into the filter visualization papers for this project -- I failed to realize that backpropagation could do exactly what I wanted!&lt;/p&gt;&lt;/div&gt;</description><category>autoencoder</category><category>caffe</category><category>deep learning</category><category>lorenzo torresani</category><category>machine learning</category><category>openFrameworks</category><category>recurrent</category><category>video feedback</category><category>visual</category><guid>https://victor-shepardson.github.io/blog/posts/convnet-video-feedback/</guid><pubDate>Mon, 09 Jan 2017 23:26:25 GMT</pubDate></item></channel></rss>